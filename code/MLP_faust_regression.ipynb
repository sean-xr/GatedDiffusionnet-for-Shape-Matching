{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26554,"status":"ok","timestamp":1704878424476,"user":{"displayName":"Rui Xiao","userId":"00023545256536816003"},"user_tz":-60},"id":"ZAxa0jY3eSgR","outputId":"1ef7929a-e594-45c7-8550-36923aeab3cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":840,"status":"ok","timestamp":1704878437418,"user":{"displayName":"Rui Xiao","userId":"00023545256536816003"},"user_tz":-60},"id":"dnect8Qii2oK","outputId":"9e2cdb55-85fa-4cf1-841e-91ce531e16be"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/P3DCV/Diff-FMAPs/code\n"]}],"source":["%cd \"/content/drive/MyDrive/P3DCV/Diff-FMAPs/code/\"\n","import sys\n","sys.path.insert(0, \"/content/drive/MyDrive/P3DCV/Diff-FMAPs/code/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56542,"status":"ok","timestamp":1704808458085,"user":{"displayName":"Rui Xiao","userId":"00023545256536816003"},"user_tz":-60},"id":"PaGxAVjdeZ1X","outputId":"0773177b-0a9a-406a-f726-624502a0432f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.3.2)\n","Collecting plyfile\n","  Downloading plyfile-1.0.3-py3-none-any.whl (23 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from plyfile) (1.23.5)\n","Installing collected packages: plyfile\n","Successfully installed plyfile-1.0.3\n","Collecting polyscope\n","  Downloading polyscope-2.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from polyscope) (1.23.5)\n","Installing collected packages: polyscope\n","Successfully installed polyscope-2.0.0\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n","Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.23.5)\n","Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.10/dist-packages (3.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (4.5.0)\n","Collecting libigl\n","  Downloading libigl-2.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.2/16.2 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from libigl) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from libigl) (1.11.4)\n","Installing collected packages: libigl\n","Successfully installed libigl-2.5.0\n","Collecting potpourri3d\n","  Downloading potpourri3d-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (869 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.3/869.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from potpourri3d) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from potpourri3d) (1.11.4)\n","Installing collected packages: potpourri3d\n","Successfully installed potpourri3d-1.0.0\n","Collecting robust_laplacian\n","  Downloading robust_laplacian-0.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (390 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.1/390.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from robust_laplacian) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from robust_laplacian) (1.11.4)\n","Installing collected packages: robust_laplacian\n","Successfully installed robust_laplacian-0.2.7\n"]}],"source":["!pip install joblib\n","!pip install plyfile\n","!pip install polyscope\n","!pip install scikit-learn\n","!pip install scipy\n","!pip install threadpoolctl\n","!pip install tqdm\n","!pip install typing-extensions\n","!pip install libigl\n","!pip install potpourri3d\n","!pip install robust_laplacian"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"52OPYE85ecb1"},"outputs":[],"source":["import igl\n","import numpy as np\n","import os\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from Diffusionnet_MPIFAUST_dataset import DiffusionnetRegressionDataset\n","from model_diffusionnet import MLP, MLP2, MLP3\n","\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J6-5H-a_Uumi"},"outputs":[],"source":["dtype = torch.float32\n","# model\n","input_features = 'xyz' # one of ['xyz', 'hks']\n","k_eig = 128\n","\n","# functional maps settings\n","n_fmap = 30 # number of eigenvectors used within functional maps\n","n_feat = 100 # dimension of features computed by DiffusionNet extractor\n","lambda_param = 1e-3 # functional map block regularization parameter\n","\n","# training settings\n","train = True\n","n_epoch = 50\n","lr = 5e-4\n","decay_every = 9999\n","decay_rate = 0.1\n","augment_random_rotate = (input_features == 'xyz')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3355,"status":"ok","timestamp":1704812584346,"user":{"displayName":"Rui Xiao","userId":"00023545256536816003"},"user_tz":-60},"id":"f1gYZnjueep1","outputId":"5d3e8253-b42a-4160-e90d-cd663558ee29"},"outputs":[{"output_type":"stream","name":"stdout","text":["training mode: True, length of dataset: 80\n","using dataset cache path: /content/drive/MyDrive/P3DCV/data/MPI-FAUST/cache/train.pt\n","  --> loading dataset from cache\n","training mode: False, length of dataset: 20\n","using dataset cache path: /content/drive/MyDrive/P3DCV/data/MPI-FAUST/cache/test.pt\n","  --> loading dataset from cache\n"]}],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","n_epoch = 300\n","lr = 0.004\n","decay_every = 9999\n","decay_rate = 0.1\n","root_dir = \"/content/drive/MyDrive/P3DCV/data/\"\n","op_cache_dir = os.path.join(root_dir, \"op_cache\")\n","#defining train and test set\n","train_set = DiffusionnetRegressionDataset(root_dir, name=\"MPI-FAUST\", train=True, k_eig=k_eig, use_cache=True, op_cache_dir=op_cache_dir)\n","train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n","test_set = DiffusionnetRegressionDataset(root_dir, name=\"MPI-FAUST\", train=False, k_eig=k_eig, use_cache=True, op_cache_dir=op_cache_dir)\n","test_loader = DataLoader(test_set, batch_size=2, shuffle=True)\n","'''\n","IMPORTANT: Model Saving\n","'''\n","model_save_dir = os.path.join(root_dir, \"models\", \"mlp\")\n","if not os.path.exists(model_save_dir):\n","  os.makedirs(model_save_dir, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AHmYTX9qehxc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704808896446,"user_tz":-60,"elapsed":4,"user":{"displayName":"Rui Xiao","userId":"00023545256536816003"}},"outputId":"c85a20da-19e4-4266-cff5-825a3591953c"},"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","verts shape torch.Size([8, 6890, 3])\n","basis shape torch.Size([8, 6890, 100])\n","1\n","verts shape torch.Size([8, 6890, 3])\n","basis shape torch.Size([8, 6890, 100])\n","2\n","verts shape torch.Size([8, 6890, 3])\n","basis shape torch.Size([8, 6890, 100])\n","3\n","verts shape torch.Size([8, 6890, 3])\n","basis shape torch.Size([8, 6890, 100])\n","4\n","verts shape torch.Size([8, 6890, 3])\n","basis shape torch.Size([8, 6890, 100])\n","5\n","verts shape torch.Size([8, 6890, 3])\n","basis shape torch.Size([8, 6890, 100])\n","6\n","verts shape torch.Size([8, 6890, 3])\n","basis shape torch.Size([8, 6890, 100])\n","7\n","verts shape torch.Size([8, 6890, 3])\n","basis shape torch.Size([8, 6890, 100])\n","8\n","verts shape torch.Size([8, 6890, 3])\n","basis shape torch.Size([8, 6890, 100])\n","9\n","verts shape torch.Size([8, 6890, 3])\n","basis shape torch.Size([8, 6890, 100])\n"]}],"source":["for i, data in enumerate(train_loader):\n","  print(i)\n","  verts, *data, basis = data\n","  print(\"verts shape\", verts.shape)\n","  print(\"basis shape\", basis.shape)"]},{"cell_type":"code","source":["class CustomLoss(nn.Module):\n","    def __init__(self, epsilon):\n","        super(CustomLoss, self).__init__()\n","        self.epsilon = epsilon\n","    def forward(self, inputs, targets):\n","        loss = torch.where(torch.abs(targets) < self.epsilon,\n","                           torch.abs(inputs),\n","                           (inputs - targets) ** 2)\n","        return torch.mean(loss)\n","custom_loss = CustomLoss(epsilon=1e-3)"],"metadata":{"id":"2hkjnX5IjJXN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TQoHDV1UGR5W","executionInfo":{"status":"ok","timestamp":1704812933304,"user_tz":-60,"elapsed":343,"user":{"displayName":"Rui Xiao","userId":"00023545256536816003"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ee8cc98d-7090-4995-edc4-1c1afcb792b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["MLP2(\n","  (MLP_mlp_layer_000): Linear(in_features=3, out_features=512, bias=True)\n","  (MLP_mlp_act_000): ReLU()\n","  (MLP_mlp_layer_001): Linear(in_features=512, out_features=512, bias=True)\n","  (MLP_mlp_act_001): ReLU()\n","  (MLP_mlp_layer_002): Linear(in_features=512, out_features=512, bias=True)\n","  (MLP_mlp_act_002): ReLU()\n","  (MLP_mlp_layer_003): Linear(in_features=512, out_features=100, bias=True)\n",")\n"]}],"source":["cfg = {\n","    \"num_layers\": 3,\n","    \"in_dim\": 3,\n","    \"h_dim\": 512,\n","    \"out_dim\": 100,\n","    \"dropout_rate\": 0.0  # Adjust the dropout rate as needed\n","}\n","\n","#model = MLP(cfg)\n","model = MLP2(layer_sizes=[3, 512, 512, 512, 100])\n","model = model.to(device)\n","optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.001)\n","l1_loss = torch.nn.L1Loss(reduction='mean')\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A5SiGhoGeGDN","outputId":"ad35706f-379a-4c43-c80b-8488f51e0e6d","executionInfo":{"status":"ok","timestamp":1704813200207,"user_tz":-60,"elapsed":261788,"user":{"displayName":"Rui Xiao","userId":"00023545256536816003"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","training loss for epoch0: 0.4342231810092926\n","validation loss for epoch0: 0.4368819028139114\n","\n","training loss for epoch1: 0.43118337392807005\n","validation loss for epoch1: 0.43681319057941437\n","\n","training loss for epoch2: 0.4311663001775742\n","validation loss for epoch2: 0.4368023365736008\n","\n","training loss for epoch3: 0.43114611208438874\n","validation loss for epoch3: 0.4367839604616165\n","\n","training loss for epoch4: 0.43113244473934176\n","validation loss for epoch4: 0.4367761164903641\n","\n","training loss for epoch5: 0.43111816644668577\n","validation loss for epoch5: 0.43674786686897277\n","\n","training loss for epoch6: 0.4310870736837387\n","validation loss for epoch6: 0.43670235872268676\n","\n","training loss for epoch7: 0.4310410678386688\n","validation loss for epoch7: 0.4366554170846939\n","\n","training loss for epoch8: 0.4309876590967178\n","validation loss for epoch8: 0.4365985631942749\n","\n","training loss for epoch9: 0.43091154396533965\n","validation loss for epoch9: 0.4364568263292313\n","\n","training loss for epoch10: 0.43075571656227113\n","validation loss for epoch10: 0.4362594485282898\n","\n","training loss for epoch11: 0.4305849462747574\n","validation loss for epoch11: 0.43611029386520384\n","\n","training loss for epoch12: 0.43052963316440584\n","validation loss for epoch12: 0.4360174387693405\n","\n","training loss for epoch13: 0.43041251599788666\n","validation loss for epoch13: 0.4357878565788269\n","\n","training loss for epoch14: 0.43019087612628937\n","validation loss for epoch14: 0.43571438193321227\n","\n","training loss for epoch15: 0.4301674246788025\n","validation loss for epoch15: 0.4356538951396942\n","\n","training loss for epoch16: 0.4301455199718475\n","validation loss for epoch16: 0.4355661362409592\n","\n","training loss for epoch17: 0.430048269033432\n","validation loss for epoch17: 0.43552019000053405\n","\n","training loss for epoch18: 0.4299405574798584\n","validation loss for epoch18: 0.43545874357223513\n","\n","training loss for epoch19: 0.42995586097240446\n","validation loss for epoch19: 0.4353739470243454\n","\n","training loss for epoch20: 0.42991267144680023\n","validation loss for epoch20: 0.43547705113887786\n","\n","training loss for epoch21: 0.42991667091846464\n","validation loss for epoch21: 0.43534092605113983\n","\n","training loss for epoch22: 0.4297595083713531\n","validation loss for epoch22: 0.4352719962596893\n","\n","training loss for epoch23: 0.42973294854164124\n","validation loss for epoch23: 0.4351710170507431\n","\n","training loss for epoch24: 0.4296518862247467\n","validation loss for epoch24: 0.4351792961359024\n","\n","training loss for epoch25: 0.42962356507778166\n","validation loss for epoch25: 0.4351168066263199\n","\n","training loss for epoch26: 0.42960608899593355\n","validation loss for epoch26: 0.435124409198761\n","\n","training loss for epoch27: 0.4296053141355515\n","validation loss for epoch27: 0.4351243764162064\n","\n","training loss for epoch28: 0.42962954342365267\n","validation loss for epoch28: 0.43525439500808716\n","\n","training loss for epoch29: 0.42950675785541537\n","validation loss for epoch29: 0.43513579964637755\n","\n","training loss for epoch30: 0.4296144187450409\n","validation loss for epoch30: 0.4352735638618469\n","\n","training loss for epoch31: 0.429520046710968\n","validation loss for epoch31: 0.43495790660381317\n","\n","training loss for epoch32: 0.42940962612628936\n","validation loss for epoch32: 0.4349386513233185\n","\n","training loss for epoch33: 0.429312127828598\n","validation loss for epoch33: 0.434869784116745\n","\n","training loss for epoch34: 0.42934649586677553\n","validation loss for epoch34: 0.43483873903751374\n","\n","training loss for epoch35: 0.42932564318180083\n","validation loss for epoch35: 0.4348886996507645\n","\n","training loss for epoch36: 0.4292853146791458\n","validation loss for epoch36: 0.43482819497585296\n","\n","training loss for epoch37: 0.4293006181716919\n","validation loss for epoch37: 0.4347401112318039\n","\n","training loss for epoch38: 0.42920060753822326\n","validation loss for epoch38: 0.43478394150733946\n","\n","training loss for epoch39: 0.4291592836380005\n","validation loss for epoch39: 0.4347131609916687\n","\n","training loss for epoch40: 0.4291568726301193\n","validation loss for epoch40: 0.43475156724452974\n","\n","training loss for epoch41: 0.4291164755821228\n","validation loss for epoch41: 0.43483902513980865\n","\n","training loss for epoch42: 0.42915544509887693\n","validation loss for epoch42: 0.4346451610326767\n","\n","training loss for epoch43: 0.4290456920862198\n","validation loss for epoch43: 0.4345678150653839\n","\n","training loss for epoch44: 0.4289522558450699\n","validation loss for epoch44: 0.43445364832878114\n","\n","training loss for epoch45: 0.4288879334926605\n","validation loss for epoch45: 0.4343952864408493\n","\n","training loss for epoch46: 0.4289165109395981\n","validation loss for epoch46: 0.4345253765583038\n","\n","training loss for epoch47: 0.42894994616508486\n","validation loss for epoch47: 0.4345057785511017\n","\n","training loss for epoch48: 0.4288762778043747\n","validation loss for epoch48: 0.43438413441181184\n","\n","training loss for epoch49: 0.4288352370262146\n","validation loss for epoch49: 0.4345126450061798\n","\n","training loss for epoch50: 0.4288427233695984\n","validation loss for epoch50: 0.43426859974861143\n","\n","training loss for epoch51: 0.4287135094404221\n","validation loss for epoch51: 0.4344355523586273\n","\n","training loss for epoch52: 0.42871035635471344\n","validation loss for epoch52: 0.4341811537742615\n","\n","training loss for epoch53: 0.42869033813476565\n","validation loss for epoch53: 0.43423170745372774\n","\n","training loss for epoch54: 0.4287739723920822\n","validation loss for epoch54: 0.4345197767019272\n","\n","training loss for epoch55: 0.4287725448608398\n","validation loss for epoch55: 0.43410204648971557\n","\n","training loss for epoch56: 0.4286716192960739\n","validation loss for epoch56: 0.43425831496715545\n","\n","training loss for epoch57: 0.42867240607738494\n","validation loss for epoch57: 0.4341780930757523\n","\n","training loss for epoch58: 0.4285830587148666\n","validation loss for epoch58: 0.43418319821357726\n","\n","training loss for epoch59: 0.4285468816757202\n","validation loss for epoch59: 0.43398132026195524\n","\n","training loss for epoch60: 0.4284807354211807\n","validation loss for epoch60: 0.4340200901031494\n","\n","training loss for epoch61: 0.4284812122583389\n","validation loss for epoch61: 0.4340719074010849\n","\n","training loss for epoch62: 0.4285586416721344\n","validation loss for epoch62: 0.4340699136257172\n","\n","training loss for epoch63: 0.4285901963710785\n","validation loss for epoch63: 0.4341290920972824\n","\n","training loss for epoch64: 0.42852762937545774\n","validation loss for epoch64: 0.4339855670928955\n","\n","training loss for epoch65: 0.4284309417009354\n","validation loss for epoch65: 0.4339397221803665\n","\n","training loss for epoch66: 0.4283923000097275\n","validation loss for epoch66: 0.4339017510414124\n","\n","training loss for epoch67: 0.42840585112571716\n","validation loss for epoch67: 0.4339424818754196\n","\n","training loss for epoch68: 0.4283659517765045\n","validation loss for epoch68: 0.4338479846715927\n","\n","training loss for epoch69: 0.428325355052948\n","validation loss for epoch69: 0.4338050037622452\n","\n","training loss for epoch70: 0.42828221917152404\n","validation loss for epoch70: 0.43377017974853516\n","\n","training loss for epoch71: 0.4282765328884125\n","validation loss for epoch71: 0.43376309871673585\n","\n","training loss for epoch72: 0.42824195325374603\n","validation loss for epoch72: 0.43367385268211367\n","\n","training loss for epoch73: 0.4282090336084366\n","validation loss for epoch73: 0.433761739730835\n","\n","training loss for epoch74: 0.42830619812011717\n","validation loss for epoch74: 0.43368795812129973\n","\n","training loss for epoch75: 0.4283883094787598\n","validation loss for epoch75: 0.43373108804225924\n","\n","training loss for epoch76: 0.4282247692346573\n","validation loss for epoch76: 0.43367337584495547\n","\n","training loss for epoch77: 0.42817655205726624\n","validation loss for epoch77: 0.4336391448974609\n","\n","training loss for epoch78: 0.4281321197748184\n","validation loss for epoch78: 0.4335738062858582\n","\n","training loss for epoch79: 0.4280966341495514\n","validation loss for epoch79: 0.4336248993873596\n","\n","training loss for epoch80: 0.42807925641536715\n","validation loss for epoch80: 0.4335486650466919\n","\n","training loss for epoch81: 0.42807101607322695\n","validation loss for epoch81: 0.4334923028945923\n","\n","training loss for epoch82: 0.42805618345737456\n","validation loss for epoch82: 0.4335115045309067\n","\n","training loss for epoch83: 0.42804913222789764\n","validation loss for epoch83: 0.43350732028484346\n","\n","training loss for epoch84: 0.42800554931163787\n","validation loss for epoch84: 0.43344723582267763\n","\n","training loss for epoch85: 0.42795721590518954\n","validation loss for epoch85: 0.4334766834974289\n","\n","training loss for epoch86: 0.428016659617424\n","validation loss for epoch86: 0.43358456492424013\n","\n","training loss for epoch87: 0.42813707888126373\n","validation loss for epoch87: 0.4335770487785339\n","\n","training loss for epoch88: 0.4280970126390457\n","validation loss for epoch88: 0.43348677158355714\n","\n","training loss for epoch89: 0.4279891014099121\n","validation loss for epoch89: 0.43343599438667296\n","\n","training loss for epoch90: 0.4280239403247833\n","validation loss for epoch90: 0.4334086388349533\n","\n","training loss for epoch91: 0.4280117958784103\n","validation loss for epoch91: 0.4334529757499695\n","\n","training loss for epoch92: 0.42793427407741547\n","validation loss for epoch92: 0.43341030478477477\n","\n","training loss for epoch93: 0.4278931528329849\n","validation loss for epoch93: 0.4333652049303055\n","\n","training loss for epoch94: 0.4279265940189362\n","validation loss for epoch94: 0.43344576060771944\n","\n","training loss for epoch95: 0.42789293825626373\n","validation loss for epoch95: 0.43326357305049895\n","\n","training loss for epoch96: 0.4278586357831955\n","validation loss for epoch96: 0.43334746062755586\n","\n","training loss for epoch97: 0.4278452545404434\n","validation loss for epoch97: 0.4333229184150696\n","\n","training loss for epoch98: 0.42784525752067565\n","validation loss for epoch98: 0.4333466351032257\n","\n","training loss for epoch99: 0.42783676981925967\n","validation loss for epoch99: 0.4332460850477219\n","\n","training loss for epoch100: 0.4278355985879898\n","validation loss for epoch100: 0.43333001136779786\n","\n","training loss for epoch101: 0.42788546681404116\n","validation loss for epoch101: 0.43333167433738706\n","\n","training loss for epoch102: 0.4278013795614243\n","validation loss for epoch102: 0.4331295400857925\n","\n","training loss for epoch103: 0.42776562869548795\n","validation loss for epoch103: 0.4332030862569809\n","\n","training loss for epoch104: 0.4277875781059265\n","validation loss for epoch104: 0.4332615822553635\n","\n","training loss for epoch105: 0.4278367131948471\n","validation loss for epoch105: 0.43324818909168245\n","\n","training loss for epoch106: 0.4278108060359955\n","validation loss for epoch106: 0.4332177132368088\n","\n","training loss for epoch107: 0.42774492800235747\n","validation loss for epoch107: 0.433176726102829\n","\n","training loss for epoch108: 0.4276911854743958\n","validation loss for epoch108: 0.4331575483083725\n","\n","training loss for epoch109: 0.4276860624551773\n","validation loss for epoch109: 0.43311641216278074\n","\n","training loss for epoch110: 0.42767504751682284\n","validation loss for epoch110: 0.4331319868564606\n","\n","training loss for epoch111: 0.42761210203170774\n","validation loss for epoch111: 0.43310267329216\n","\n","training loss for epoch112: 0.427780357003212\n","validation loss for epoch112: 0.4330834597349167\n","\n","training loss for epoch113: 0.42768012881278994\n","validation loss for epoch113: 0.4331279188394547\n","\n","training loss for epoch114: 0.42765032947063447\n","validation loss for epoch114: 0.43307086527347566\n","\n","training loss for epoch115: 0.4277052074670792\n","validation loss for epoch115: 0.4331923395395279\n","\n","training loss for epoch116: 0.42764127552509307\n","validation loss for epoch116: 0.43311347663402555\n","\n","training loss for epoch117: 0.42768786251544955\n","validation loss for epoch117: 0.4330504655838013\n","\n","training loss for epoch118: 0.42771861851215365\n","validation loss for epoch118: 0.43323110342025756\n","\n","training loss for epoch119: 0.4277151614427567\n","validation loss for epoch119: 0.4331980854272842\n","\n","training loss for epoch120: 0.4277398258447647\n","validation loss for epoch120: 0.4331971317529678\n","\n","training loss for epoch121: 0.4277299076318741\n","validation loss for epoch121: 0.4332512766122818\n","\n","training loss for epoch122: 0.4276926130056381\n","validation loss for epoch122: 0.43296924233436584\n","\n","training loss for epoch123: 0.42758928537368773\n","validation loss for epoch123: 0.43299299478530884\n","\n","training loss for epoch124: 0.4275927782058716\n","validation loss for epoch124: 0.4330151855945587\n","\n","training loss for epoch125: 0.4275525838136673\n","validation loss for epoch125: 0.43307397365570066\n","\n","training loss for epoch126: 0.4276075094938278\n","validation loss for epoch126: 0.43302516639232635\n","\n","training loss for epoch127: 0.4275147944688797\n","validation loss for epoch127: 0.4329261541366577\n","\n","training loss for epoch128: 0.42748438119888305\n","validation loss for epoch128: 0.43305407762527465\n","\n","training loss for epoch129: 0.42755517065525056\n","validation loss for epoch129: 0.4330346673727036\n","\n","training loss for epoch130: 0.4274852961301804\n","validation loss for epoch130: 0.43310731947422026\n","\n","training loss for epoch131: 0.4274750083684921\n","validation loss for epoch131: 0.432976695895195\n","\n","training loss for epoch132: 0.4274805516004562\n","validation loss for epoch132: 0.4330132991075516\n","\n","training loss for epoch133: 0.42750624418258665\n","validation loss for epoch133: 0.43291022479534147\n","\n","training loss for epoch134: 0.4274593234062195\n","validation loss for epoch134: 0.43296690583229064\n","\n","training loss for epoch135: 0.4275027185678482\n","validation loss for epoch135: 0.43291031420230863\n","\n","training loss for epoch136: 0.42744810283184054\n","validation loss for epoch136: 0.43294893205165863\n","\n","training loss for epoch137: 0.4274866312742233\n","validation loss for epoch137: 0.4329760938882828\n","\n","training loss for epoch138: 0.42752436697483065\n","validation loss for epoch138: 0.4329141050577164\n","\n","training loss for epoch139: 0.42748457193374634\n","validation loss for epoch139: 0.43295691907405853\n","\n","training loss for epoch140: 0.42747868597507477\n","validation loss for epoch140: 0.433003169298172\n","\n","training loss for epoch141: 0.42749368846416474\n","validation loss for epoch141: 0.4329588025808334\n","\n","training loss for epoch142: 0.42742458283901213\n","validation loss for epoch142: 0.4329770028591156\n","\n","training loss for epoch143: 0.42743084728717806\n","validation loss for epoch143: 0.4328604847192764\n","\n","training loss for epoch144: 0.42745156586170197\n","validation loss for epoch144: 0.4330387979745865\n","\n","training loss for epoch145: 0.42744789123535154\n","validation loss for epoch145: 0.43286262452602386\n","\n","training loss for epoch146: 0.4274059236049652\n","validation loss for epoch146: 0.4327985256910324\n","\n","training loss for epoch147: 0.4274631142616272\n","validation loss for epoch147: 0.4329421311616898\n","\n","training loss for epoch148: 0.4274136543273926\n","validation loss for epoch148: 0.43286107778549193\n","\n","training loss for epoch149: 0.42738797068595885\n","validation loss for epoch149: 0.4327640235424042\n","\n","training loss for epoch150: 0.4273992419242859\n","validation loss for epoch150: 0.4328866690397263\n","\n","training loss for epoch151: 0.4273657649755478\n","validation loss for epoch151: 0.4327668070793152\n","\n","training loss for epoch152: 0.42734605073928833\n","validation loss for epoch152: 0.4327193021774292\n","\n","training loss for epoch153: 0.42736061215400695\n","validation loss for epoch153: 0.43285691142082217\n","\n","training loss for epoch154: 0.4273965448141098\n","validation loss for epoch154: 0.43281010389328\n","\n","training loss for epoch155: 0.42738659381866456\n","validation loss for epoch155: 0.4328246831893921\n","\n","training loss for epoch156: 0.4273336559534073\n","validation loss for epoch156: 0.43289336264133454\n","\n","training loss for epoch157: 0.42736147046089173\n","validation loss for epoch157: 0.4329289227724075\n","\n","training loss for epoch158: 0.42732344269752504\n","validation loss for epoch158: 0.4328865647315979\n","\n","training loss for epoch159: 0.427322182059288\n","validation loss for epoch159: 0.4327401101589203\n","\n","training loss for epoch160: 0.4273096114397049\n","validation loss for epoch160: 0.43271878361701965\n","\n","training loss for epoch161: 0.42730327546596525\n","validation loss for epoch161: 0.4327414631843567\n","\n","training loss for epoch162: 0.4273087829351425\n","validation loss for epoch162: 0.4328011810779572\n","\n","training loss for epoch163: 0.4272464752197266\n","validation loss for epoch163: 0.432761886715889\n","\n","training loss for epoch164: 0.4272830545902252\n","validation loss for epoch164: 0.4326906144618988\n","\n","training loss for epoch165: 0.4272619545459747\n","validation loss for epoch165: 0.4327558845281601\n","\n","training loss for epoch166: 0.4272436022758484\n","validation loss for epoch166: 0.4327983856201172\n","\n","training loss for epoch167: 0.4273404270410538\n","validation loss for epoch167: 0.43274460434913636\n","\n","training loss for epoch168: 0.4274179995059967\n","validation loss for epoch168: 0.4327956348657608\n","\n","training loss for epoch169: 0.4273018538951874\n","validation loss for epoch169: 0.43272490203380587\n","\n","training loss for epoch170: 0.42729737162590026\n","validation loss for epoch170: 0.4326722979545593\n","\n","training loss for epoch171: 0.42730394899845126\n","validation loss for epoch171: 0.43268260955810545\n","\n","training loss for epoch172: 0.4272702246904373\n","validation loss for epoch172: 0.4327292501926422\n","\n","training loss for epoch173: 0.4272182911634445\n","validation loss for epoch173: 0.43273867666721344\n","\n","training loss for epoch174: 0.4272334188222885\n","validation loss for epoch174: 0.4326752215623856\n","\n","training loss for epoch175: 0.4272375196218491\n","validation loss for epoch175: 0.4328201740980148\n","\n","training loss for epoch176: 0.4272211313247681\n","validation loss for epoch176: 0.4327007681131363\n","\n","training loss for epoch177: 0.4272043466567993\n","validation loss for epoch177: 0.4325465261936188\n","\n","training loss for epoch178: 0.4272001266479492\n","validation loss for epoch178: 0.4326926529407501\n","\n","training loss for epoch179: 0.4272375673055649\n","validation loss for epoch179: 0.43260191977024076\n","\n","training loss for epoch180: 0.4271866112947464\n","validation loss for epoch180: 0.4326940178871155\n","\n","training loss for epoch181: 0.4271932065486908\n","validation loss for epoch181: 0.43274053037166593\n","\n","training loss for epoch182: 0.4272854536771774\n","validation loss for epoch182: 0.4326928347349167\n","\n","training loss for epoch183: 0.4272275745868683\n","validation loss for epoch183: 0.43265892565250397\n","\n","training loss for epoch184: 0.4272245913743973\n","validation loss for epoch184: 0.4327780455350876\n","\n","training loss for epoch185: 0.4272031605243683\n","validation loss for epoch185: 0.4327154755592346\n","\n","training loss for epoch186: 0.42720149755477904\n","validation loss for epoch186: 0.4327574670314789\n","\n","training loss for epoch187: 0.42723816335201265\n","validation loss for epoch187: 0.4326570421457291\n","\n","training loss for epoch188: 0.42719221115112305\n","validation loss for epoch188: 0.43254846930503843\n","\n","training loss for epoch189: 0.4271757334470749\n","validation loss for epoch189: 0.4326655089855194\n","\n","training loss for epoch190: 0.4272147357463837\n","validation loss for epoch190: 0.43263583183288573\n","\n","training loss for epoch191: 0.4272224962711334\n","validation loss for epoch191: 0.43261373341083526\n","\n","training loss for epoch192: 0.42719379961490633\n","validation loss for epoch192: 0.4325371295213699\n","\n","training loss for epoch193: 0.42714197337627413\n","validation loss for epoch193: 0.4325833320617676\n","\n","training loss for epoch194: 0.4271716207265854\n","validation loss for epoch194: 0.4326121836900711\n","\n","training loss for epoch195: 0.42715027630329133\n","validation loss for epoch195: 0.43264987468719485\n","\n","training loss for epoch196: 0.4271638035774231\n","validation loss for epoch196: 0.43259797990322113\n","\n","training loss for epoch197: 0.4272519886493683\n","validation loss for epoch197: 0.43263699412345885\n","\n","training loss for epoch198: 0.42719072103500366\n","validation loss for epoch198: 0.4325508654117584\n","\n","training loss for epoch199: 0.42713840007781984\n","validation loss for epoch199: 0.4326385408639908\n","\n","training loss for epoch200: 0.42712526917457583\n","validation loss for epoch200: 0.43254154920578003\n","\n","training loss for epoch201: 0.42711299657821655\n","validation loss for epoch201: 0.43255193531513214\n","\n","training loss for epoch202: 0.4270726889371872\n","validation loss for epoch202: 0.4325277715921402\n","\n","training loss for epoch203: 0.4271189272403717\n","validation loss for epoch203: 0.43258817195892335\n","\n","training loss for epoch204: 0.42709671556949613\n","validation loss for epoch204: 0.4326000899076462\n","\n","training loss for epoch205: 0.4270769268274307\n","validation loss for epoch205: 0.43249608874320983\n","\n","training loss for epoch206: 0.4270740538835526\n","validation loss for epoch206: 0.4325416773557663\n","\n","training loss for epoch207: 0.4270853757858276\n","validation loss for epoch207: 0.4324743151664734\n","\n","training loss for epoch208: 0.4270636081695557\n","validation loss for epoch208: 0.432508584856987\n","\n","training loss for epoch209: 0.4270394295454025\n","validation loss for epoch209: 0.43250301778316497\n","\n","training loss for epoch210: 0.42705428302288057\n","validation loss for epoch210: 0.4324188232421875\n","\n","training loss for epoch211: 0.42708088755607604\n","validation loss for epoch211: 0.43256257474422455\n","\n","training loss for epoch212: 0.4270929455757141\n","validation loss for epoch212: 0.4326703041791916\n","\n","training loss for epoch213: 0.4270973354578018\n","validation loss for epoch213: 0.43266758918762205\n","\n","training loss for epoch214: 0.42703816294670105\n","validation loss for epoch214: 0.43254167437553404\n","\n","training loss for epoch215: 0.427036440372467\n","validation loss for epoch215: 0.43248334527015686\n","\n","training loss for epoch216: 0.42702907621860503\n","validation loss for epoch216: 0.43252149820327757\n","\n","training loss for epoch217: 0.427079638838768\n","validation loss for epoch217: 0.4325581520795822\n","\n","training loss for epoch218: 0.427016544342041\n","validation loss for epoch218: 0.43255450427532194\n","\n","training loss for epoch219: 0.42711590230464935\n","validation loss for epoch219: 0.4324611246585846\n","\n","training loss for epoch220: 0.4270601123571396\n","validation loss for epoch220: 0.432478728890419\n","\n","training loss for epoch221: 0.4271079123020172\n","validation loss for epoch221: 0.4325045675039291\n","\n","training loss for epoch222: 0.4270840257406235\n","validation loss for epoch222: 0.4324475109577179\n","\n","training loss for epoch223: 0.4270508050918579\n","validation loss for epoch223: 0.43250963687896726\n","\n","training loss for epoch224: 0.42702214121818544\n","validation loss for epoch224: 0.4324744254350662\n","\n","training loss for epoch225: 0.4270416796207428\n","validation loss for epoch225: 0.43252783715724946\n","\n","training loss for epoch226: 0.427067294716835\n","validation loss for epoch226: 0.4324753910303116\n","\n","training loss for epoch227: 0.427098947763443\n","validation loss for epoch227: 0.432719761133194\n","\n","training loss for epoch228: 0.4271088749170303\n","validation loss for epoch228: 0.432573065161705\n","\n","training loss for epoch229: 0.4270348191261292\n","validation loss for epoch229: 0.43241083025932314\n","\n","training loss for epoch230: 0.42701431810855867\n","validation loss for epoch230: 0.4325030565261841\n","\n","training loss for epoch231: 0.4270296037197113\n","validation loss for epoch231: 0.43244213461875913\n","\n","training loss for epoch232: 0.42702639400959014\n","validation loss for epoch232: 0.4325124740600586\n","\n","training loss for epoch233: 0.4269861042499542\n","validation loss for epoch233: 0.4323757469654083\n","\n","training loss for epoch234: 0.4269706279039383\n","validation loss for epoch234: 0.4324355959892273\n","\n","training loss for epoch235: 0.4269163519144058\n","validation loss for epoch235: 0.43232548534870147\n","\n","training loss for epoch236: 0.4269394129514694\n","validation loss for epoch236: 0.43243443965911865\n","\n","training loss for epoch237: 0.426973882317543\n","validation loss for epoch237: 0.43255141079425813\n","\n","training loss for epoch238: 0.4269965201616287\n","validation loss for epoch238: 0.4323724448680878\n","\n","training loss for epoch239: 0.42699649930000305\n","validation loss for epoch239: 0.4324149310588837\n","\n","training loss for epoch240: 0.4269979238510132\n","validation loss for epoch240: 0.4323933184146881\n","\n","training loss for epoch241: 0.42693363428115844\n","validation loss for epoch241: 0.43252389430999755\n","\n","training loss for epoch242: 0.4270251214504242\n","validation loss for epoch242: 0.43253434300422666\n","\n","training loss for epoch243: 0.42710529565811156\n","validation loss for epoch243: 0.4324623614549637\n","\n","training loss for epoch244: 0.4270659744739532\n","validation loss for epoch244: 0.432388511300087\n","\n","training loss for epoch245: 0.42704171538352964\n","validation loss for epoch245: 0.4324962258338928\n","\n","training loss for epoch246: 0.4269211500883102\n","validation loss for epoch246: 0.4324076443910599\n","\n","training loss for epoch247: 0.42694960832595824\n","validation loss for epoch247: 0.43243777453899385\n","\n","training loss for epoch248: 0.4270378440618515\n","validation loss for epoch248: 0.43242070972919466\n","\n","training loss for epoch249: 0.4269842177629471\n","validation loss for epoch249: 0.43229915499687194\n","\n","training loss for epoch250: 0.42693050801753996\n","validation loss for epoch250: 0.4323518395423889\n","\n","training loss for epoch251: 0.4269586980342865\n","validation loss for epoch251: 0.43242170512676237\n","\n","training loss for epoch252: 0.42694956362247466\n","validation loss for epoch252: 0.43247403800487516\n","\n","training loss for epoch253: 0.42697549760341647\n","validation loss for epoch253: 0.4323130637407303\n","\n","training loss for epoch254: 0.4269537806510925\n","validation loss for epoch254: 0.43243975937366486\n","\n","training loss for epoch255: 0.42697564959526063\n","validation loss for epoch255: 0.4325817435979843\n","\n","training loss for epoch256: 0.4271059900522232\n","validation loss for epoch256: 0.4323955625295639\n","\n","training loss for epoch257: 0.4270444452762604\n","validation loss for epoch257: 0.43244041204452516\n","\n","training loss for epoch258: 0.4269615292549133\n","validation loss for epoch258: 0.4323700577020645\n","\n","training loss for epoch259: 0.42692078948020934\n","validation loss for epoch259: 0.43234304189682005\n","\n","training loss for epoch260: 0.4268812447786331\n","validation loss for epoch260: 0.4324637889862061\n","\n","training loss for epoch261: 0.4268946975469589\n","validation loss for epoch261: 0.4324276089668274\n","\n","training loss for epoch262: 0.42694599330425265\n","validation loss for epoch262: 0.43232239186763766\n","\n","training loss for epoch263: 0.42689278423786164\n","validation loss for epoch263: 0.4323538541793823\n","\n","training loss for epoch264: 0.42694145143032075\n","validation loss for epoch264: 0.4324096769094467\n","\n","training loss for epoch265: 0.42686561942100526\n","validation loss for epoch265: 0.43249078989028933\n","\n","training loss for epoch266: 0.42691347002983093\n","validation loss for epoch266: 0.43234820365905763\n","\n","training loss for epoch267: 0.4268826812505722\n","validation loss for epoch267: 0.432292166352272\n","\n","training loss for epoch268: 0.42686952352523805\n","validation loss for epoch268: 0.43236387968063356\n","\n","training loss for epoch269: 0.42692441642284396\n","validation loss for epoch269: 0.43242884874343873\n","\n","training loss for epoch270: 0.4269049555063248\n","validation loss for epoch270: 0.43228561878204347\n","\n","training loss for epoch271: 0.426865828037262\n","validation loss for epoch271: 0.4322613447904587\n","\n","training loss for epoch272: 0.4268560498952866\n","validation loss for epoch272: 0.43230890333652494\n","\n","training loss for epoch273: 0.42691433131694795\n","validation loss for epoch273: 0.4324897527694702\n","\n","training loss for epoch274: 0.4269083470106125\n","validation loss for epoch274: 0.4323990136384964\n","\n","training loss for epoch275: 0.42695205807685854\n","validation loss for epoch275: 0.4323647230863571\n","\n","training loss for epoch276: 0.4268752455711365\n","validation loss for epoch276: 0.43228626549243926\n","\n","training loss for epoch277: 0.4269044518470764\n","validation loss for epoch277: 0.43234647810459137\n","\n","training loss for epoch278: 0.4268560647964478\n","validation loss for epoch278: 0.43236561119556427\n","\n","training loss for epoch279: 0.4268804550170898\n","validation loss for epoch279: 0.43231076300144194\n","\n","training loss for epoch280: 0.42685002982616427\n","validation loss for epoch280: 0.4322830140590668\n","\n","training loss for epoch281: 0.42685195803642273\n","validation loss for epoch281: 0.432296684384346\n","\n","training loss for epoch282: 0.4269106239080429\n","validation loss for epoch282: 0.4324115127325058\n","\n","training loss for epoch283: 0.4269322782754898\n","validation loss for epoch283: 0.4323886841535568\n","\n","training loss for epoch284: 0.4269389182329178\n","validation loss for epoch284: 0.43234131634235384\n","\n","training loss for epoch285: 0.42690386176109313\n","validation loss for epoch285: 0.4323247104883194\n","\n","training loss for epoch286: 0.4269064486026764\n","validation loss for epoch286: 0.43244888484477995\n","\n","training loss for epoch287: 0.4268533647060394\n","validation loss for epoch287: 0.4322925120592117\n","\n","training loss for epoch288: 0.4268660545349121\n","validation loss for epoch288: 0.4322444677352905\n","\n","training loss for epoch289: 0.42678755819797515\n","validation loss for epoch289: 0.4322474986314774\n","\n","training loss for epoch290: 0.426838344335556\n","validation loss for epoch290: 0.4323335558176041\n","\n","training loss for epoch291: 0.42690642178058624\n","validation loss for epoch291: 0.43243348300457\n","\n","training loss for epoch292: 0.4269509851932526\n","validation loss for epoch292: 0.43240281045436857\n","\n","training loss for epoch293: 0.426912197470665\n","validation loss for epoch293: 0.4324504166841507\n","\n","training loss for epoch294: 0.4268851101398468\n","validation loss for epoch294: 0.4324013113975525\n","\n","training loss for epoch295: 0.4268496334552765\n","validation loss for epoch295: 0.43228243589401244\n","\n","training loss for epoch296: 0.42684025764465333\n","validation loss for epoch296: 0.43234110474586485\n","\n","training loss for epoch297: 0.4268385648727417\n","validation loss for epoch297: 0.43224546015262605\n","\n","training loss for epoch298: 0.4267692118883133\n","validation loss for epoch298: 0.4322045624256134\n","\n","training loss for epoch299: 0.42679721117019653\n","validation loss for epoch299: 0.4322037100791931\n"]}],"source":["#single head training loop\n","\n","epoch_train_loss = []\n","epoch_val_loss = []\n","for epoch in range(n_epoch):\n","    train_losses = []\n","    train_elastic_losses = []\n","    train_total_losses = []\n","    val_losses = []\n","    val_elastic_losses = []\n","    val_total_losses = []\n","    faust_losses = []\n","    #metrics = []\n","    # Training single Epoch\n","    model.train()\n","    for i, data in enumerate(train_loader):\n","        verts, *data, basis = data\n","        verts = verts.to(device)\n","        basis = basis.to(device).to(torch.float32)\n","        pred_basis = model(verts)\n","        optim.zero_grad()\n","\n","        # Custom Loss\n","        loss = l1_loss(pred_basis, basis)\n","        loss.backward()\n","        #eucl_loss.backward()\n","        optim.step()\n","        train_losses.append(loss.detach().item())\n","        #train_elastic_losses.append(elastic_loss.detach().item())\n","        #metrics.append(metric.detach().item())\n","\n","    model.eval()\n","    with torch.no_grad():\n","      for i, data in enumerate(test_loader):\n","          verts, *data, basis = data\n","          verts = verts.to(device)\n","          basis = basis.to(device).to(torch.float32)\n","          pred_basis = model(verts)\n","\n","          val_loss = l1_loss(pred_basis.detach(), basis.detach())\n","          val_losses.append(val_loss.detach().item())\n","\n","    ave_train_loss = sum(train_losses) / len(train_losses)\n","    ave_val_loss = sum(val_losses) / len(val_losses)\n","    #ave_metric = sum(metrics) / len(metrics)\n","    print()\n","    print(f\"training loss for epoch{epoch}:\", ave_train_loss)\n","    print(f\"validation loss for epoch{epoch}:\", ave_val_loss)\n","    #print(f\"training metric for epoch{epoch}:\", ave_metric)\n","    #torch.save(model.state_dict(), os.path.join(model_save_dir, f\"train_k_{n_feat}_epoch_{epoch}\"))\n","    epoch_train_loss.append(ave_train_loss)\n","    epoch_val_loss.append(ave_val_loss)\n","#np.save(f\"/content/drive/MyDrive/P3DCV/losses/train/mlp_k_{n_feat}.npy\", np.array(epoch_train_loss))\n","#np.save(f\"/content/drive/MyDrive/P3DCV/losses/val/mlp_k_{n_feat}.npy\", np.array(epoch_val_loss))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1088744,"status":"error","timestamp":1704238883899,"user":{"displayName":"Rui Xiao","userId":"00023545256536816003"},"user_tz":-60},"id":"FDpGiE2HG6Lk","outputId":"ad2a5348-1fb9-41b7-f12c-a6ab378bd877"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","training eucl loss for epoch0: 418.37023935438714\n","training elastic loss for epoch0: 0.8814719086960902\n","training total loss for epoch0: 418.5465338791473\n","validation eucl loss for epoch0: 40.53102505332545\n","validation elastic loss for epoch0: 0.8309022859523171\n","validation total loss for epoch0: 40.69720555355674\n","\n","training eucl loss for epoch1: 89.80323909324936\n","training elastic loss for epoch1: 0.8643974907790558\n","training total loss for epoch1: 89.97611844992336\n","validation eucl loss for epoch1: 46.2820611652575\n","validation elastic loss for epoch1: 0.8302119725628903\n","validation total loss for epoch1: 46.448103693911904\n","\n","training eucl loss for epoch2: 70.85475842439676\n","training elastic loss for epoch2: 0.8617582156688353\n","training total loss for epoch2: 71.02711002977588\n","validation eucl loss for epoch2: 47.74801370721114\n","validation elastic loss for epoch2: 0.8298683831566258\n","validation total loss for epoch2: 47.91398712961297\n","\n","training eucl loss for epoch3: 60.70216915275477\n","training elastic loss for epoch3: 0.8612159323088731\n","training total loss for epoch3: 60.874412401416635\n","validation eucl loss for epoch3: 43.72523080926192\n","validation elastic loss for epoch3: 0.829576419529162\n","validation total loss for epoch3: 43.8911461679559\n","\n","training eucl loss for epoch4: 54.03796826133245\n","training elastic loss for epoch4: 0.8609449932846842\n","training total loss for epoch4: 54.21015721574614\n","validation eucl loss for epoch4: 39.78726706253855\n","validation elastic loss for epoch4: 0.8291774661917436\n","validation total loss for epoch4: 39.95310247320878\n","\n","training eucl loss for epoch5: 49.13776650730568\n","training elastic loss for epoch5: 0.8607559053203727\n","training total loss for epoch5: 49.3099176431004\n","validation eucl loss for epoch5: 38.131081470690276\n","validation elastic loss for epoch5: 0.8292050606326054\n","validation total loss for epoch5: 38.296922262091385\n","\n","training eucl loss for epoch6: 45.37865080048766\n","training elastic loss for epoch6: 0.8606032564670225\n","training total loss for epoch6: 45.550771447676645\n","validation eucl loss for epoch6: 38.0396693581029\n","validation elastic loss for epoch6: 0.8289804985648708\n","validation total loss for epoch6: 38.20546541715923\n","\n","training eucl loss for epoch7: 42.49653074047234\n","training elastic loss for epoch7: 0.8604480566857736\n","training total loss for epoch7: 42.66862029063551\n","validation eucl loss for epoch7: 36.37964953372353\n","validation elastic loss for epoch7: 0.8287257338825025\n","validation total loss for epoch7: 36.54539461637798\n","\n","training eucl loss for epoch8: 39.891880383069\n","training elastic loss for epoch8: 0.8602945993218241\n","training total loss for epoch8: 40.06393931907944\n","validation eucl loss for epoch8: 36.20687713623047\n","validation elastic loss for epoch8: 0.8286796877258702\n","validation total loss for epoch8: 36.37261308368883\n","\n","training eucl loss for epoch9: 37.62498478949824\n","training elastic loss for epoch9: 0.8601621810393998\n","training total loss for epoch9: 37.79701726406435\n","validation eucl loss for epoch9: 36.139022525988125\n","validation elastic loss for epoch9: 0.8284997482048838\n","validation total loss for epoch9: 36.30472251490543\n","\n","training eucl loss for epoch10: 35.62128886935077\n","training elastic loss for epoch10: 0.8600494701651078\n","training total loss for epoch10: 35.793298726142204\n","validation eucl loss for epoch10: 36.34485708537855\n","validation elastic loss for epoch10: 0.8284644440600747\n","validation total loss for epoch10: 36.51055004722193\n","\n","training eucl loss for epoch11: 33.763551668577556\n","training elastic loss for epoch11: 0.8599623820449732\n","training total loss for epoch11: 33.93554405019253\n","validation eucl loss for epoch11: 35.72729423924496\n","validation elastic loss for epoch11: 0.8284897070181997\n","validation total loss for epoch11: 35.89299205980803\n","\n","training eucl loss for epoch12: 32.0851769217962\n","training elastic loss for epoch12: 0.859882958025872\n","training total loss for epoch12: 32.257153479660616\n","validation eucl loss for epoch12: 34.1590830351177\n","validation elastic loss for epoch12: 0.8285920387820194\n","validation total loss for epoch12: 34.324801455046\n","\n","training eucl loss for epoch13: 30.325348344633852\n","training elastic loss for epoch13: 0.8598084259636795\n","training total loss for epoch13: 30.497310049322586\n","validation eucl loss for epoch13: 33.48899728875411\n","validation elastic loss for epoch13: 0.8283376122775831\n","validation total loss for epoch13: 33.65466481258995\n","\n","training eucl loss for epoch14: 28.746112065375605\n","training elastic loss for epoch14: 0.8597517193118228\n","training total loss for epoch14: 28.918062437033353\n","validation eucl loss for epoch14: 33.9513442591617\n","validation elastic loss for epoch14: 0.8283351672323127\n","validation total loss for epoch14: 34.11701128106368\n","\n","training eucl loss for epoch15: 27.418721148333972\n","training elastic loss for epoch15: 0.859699611271484\n","training total loss for epoch15: 27.59066107061845\n","validation eucl loss for epoch15: 32.70558919404682\n","validation elastic loss for epoch15: 0.8282765162618537\n","validation total loss for epoch15: 32.87124445061934\n","\n","training eucl loss for epoch16: 26.160536391825616\n","training elastic loss for epoch16: 0.8596469957617264\n","training total loss for epoch16: 26.332465782648402\n","validation eucl loss for epoch16: 31.569006407888313\n","validation elastic loss for epoch16: 0.8281349671514411\n","validation total loss for epoch16: 31.734633315236945\n","\n","training eucl loss for epoch17: 25.183780134176907\n","training elastic loss for epoch17: 0.859617947174024\n","training total loss for epoch17: 25.355703677406794\n","validation eucl loss for epoch17: 30.944720760144687\n","validation elastic loss for epoch17: 0.8282608038500736\n","validation total loss for epoch17: 31.110373025191457\n","\n","training eucl loss for epoch18: 24.359628397301783\n","training elastic loss for epoch18: 0.8595766366282596\n","training total loss for epoch18: 24.531543683402145\n","validation eucl loss for epoch18: 30.295172078985917\n","validation elastic loss for epoch18: 0.8279301624549062\n","validation total loss for epoch18: 30.4607581289191\n","\n","training eucl loss for epoch19: 23.646697457229035\n","training elastic loss for epoch19: 0.8595155263248878\n","training total loss for epoch19: 23.818600526640687\n","validation eucl loss for epoch19: 30.907256015978362\n","validation elastic loss for epoch19: 0.8279885662229438\n","validation total loss for epoch19: 31.0728537910863\n","\n","training eucl loss for epoch20: 22.8157078320467\n","training elastic loss for epoch20: 0.859456066391136\n","training total loss for epoch20: 22.98759900346587\n","validation eucl loss for epoch20: 30.285586748625104\n","validation elastic loss for epoch20: 0.828100523823186\n","validation total loss for epoch20: 30.451206769441303\n","\n","training eucl loss for epoch21: 22.215364118165606\n","training elastic loss for epoch21: 0.8594130971763707\n","training total loss for epoch21: 22.38724670893029\n","validation eucl loss for epoch21: 29.886348222431383\n","validation elastic loss for epoch21: 0.8280295233977468\n","validation total loss for epoch21: 30.05195416902241\n","\n","training eucl loss for epoch22: 21.596927435186846\n","training elastic loss for epoch22: 0.8594046989573708\n","training total loss for epoch22: 21.768808369696895\n","validation eucl loss for epoch22: 29.637691196642425\n","validation elastic loss for epoch22: 0.8280110861125746\n","validation total loss for epoch22: 29.8032934088456\n","\n","training eucl loss for epoch23: 21.15332050564923\n","training elastic loss for epoch23: 0.8593294095389451\n","training total loss for epoch23: 21.325186369690712\n","validation eucl loss for epoch23: 28.970048362330388\n","validation elastic loss for epoch23: 0.8278086505438152\n","validation total loss for epoch23: 29.13561003835578\n","\n","training eucl loss for epoch24: 20.675950540470172\n","training elastic loss for epoch24: 0.8592939013167272\n","training total loss for epoch24: 20.847809311106236\n","validation eucl loss for epoch24: 29.200899445383172\n","validation elastic loss for epoch24: 0.8278294789163689\n","validation total loss for epoch24: 29.366465357730263\n","\n","training eucl loss for epoch25: 20.269250985640515\n","training elastic loss for epoch25: 0.8592545364476457\n","training total loss for epoch25: 20.441101914417896\n","validation eucl loss for epoch25: 29.228255783884148\n","validation elastic loss for epoch25: 0.8278329472792776\n","validation total loss for epoch25: 29.393822419015986\n","\n","training eucl loss for epoch26: 19.830474785913395\n","training elastic loss for epoch26: 0.8592170153992086\n","training total loss for epoch26: 20.00231812392609\n","validation eucl loss for epoch26: 28.6194204029284\n","validation elastic loss for epoch26: 0.8278155320569088\n","validation total loss for epoch26: 28.784983484368574\n","\n","training eucl loss for epoch27: 19.47758546177345\n","training elastic loss for epoch27: 0.8591737351840055\n","training total loss for epoch27: 19.649420170844355\n","validation eucl loss for epoch27: 29.263835465280632\n","validation elastic loss for epoch27: 0.8277338874967475\n","validation total loss for epoch27: 29.429382123445208\n","\n","training eucl loss for epoch28: 19.036202186874196\n","training elastic loss for epoch28: 0.8591339779805534\n","training total loss for epoch28: 19.208028943025614\n","validation eucl loss for epoch28: 29.655393841392115\n","validation elastic loss for epoch28: 0.8276929146365115\n","validation total loss for epoch28: 29.820932488692435\n","\n","training eucl loss for epoch29: 18.70044376518153\n","training elastic loss for epoch29: 0.8591132124768027\n","training total loss for epoch29: 18.8722664265693\n","validation eucl loss for epoch29: 28.782049420005396\n","validation elastic loss for epoch29: 0.8276756236427709\n","validation total loss for epoch29: 28.947584473459344\n","\n","training eucl loss for epoch30: 18.37692175756527\n","training elastic loss for epoch30: 0.8590810043902337\n","training total loss for epoch30: 18.548737929138955\n","validation eucl loss for epoch30: 28.90826385899594\n","validation elastic loss for epoch30: 0.8276924879927384\n","validation total loss for epoch30: 29.073802345677425\n","\n","training eucl loss for epoch31: 18.520229513433915\n","training elastic loss for epoch31: 0.8590718453443503\n","training total loss for epoch31: 18.692043898377236\n","validation eucl loss for epoch31: 29.090796520835475\n","validation elastic loss for epoch31: 0.8276056277124505\n","validation total loss for epoch31: 29.25631762052837\n","\n","training eucl loss for epoch32: 17.87868674555911\n","training elastic loss for epoch32: 0.859044966063922\n","training total loss for epoch32: 18.050495717495302\n","validation eucl loss for epoch32: 29.42632418180767\n","validation elastic loss for epoch32: 0.827672436990236\n","validation total loss for epoch32: 29.591858632940994\n","\n","training eucl loss for epoch33: 17.568571636344814\n","training elastic loss for epoch33: 0.8590183839013305\n","training total loss for epoch33: 17.740375349793254\n","validation eucl loss for epoch33: 29.068668827257657\n","validation elastic loss for epoch33: 0.8276172844987166\n","validation total loss for epoch33: 29.234192255923624\n","\n","training eucl loss for epoch34: 17.339057318771943\n","training elastic loss for epoch34: 0.8589955793151373\n","training total loss for epoch34: 17.510856415953818\n","validation eucl loss for epoch34: 29.333740796540912\n","validation elastic loss for epoch34: 0.8277635850404438\n","validation total loss for epoch34: 29.49929333737022\n","\n","training eucl loss for epoch35: 17.18019088551968\n","training elastic loss for epoch35: 0.8589671033847182\n","training total loss for epoch35: 17.351984330672252\n","validation eucl loss for epoch35: 28.726801119352643\n","validation elastic loss for epoch35: 0.8276121453235024\n","validation total loss for epoch35: 28.892323704769737\n","\n","training eucl loss for epoch36: 16.975176304201536\n","training elastic loss for epoch36: 0.8589596369598486\n","training total loss for epoch36: 17.146968269348143\n","validation eucl loss for epoch36: 28.28749616522538\n","validation elastic loss for epoch36: 0.8275507462652106\n","validation total loss for epoch36: 28.453006242450915\n","\n","training eucl loss for epoch37: 16.739031924477107\n","training elastic loss for epoch37: 0.8589088815677015\n","training total loss for epoch37: 16.910813667200788\n","validation eucl loss for epoch37: 28.534419451261822\n","validation elastic loss for epoch37: 0.8274614779572738\n","validation total loss for epoch37: 28.699911719874333\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-7ef971cc70ec>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Back Prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meucl_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0melastic_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;31m#eucl_loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Multi-head Training Loop\n","epoch_train_loss = []\n","epoch_val_loss = []\n","for epoch in range(n_epoch):\n","    train_losses = []\n","    train_elastic_losses = []\n","    train_total_losses = []\n","    val_losses = []\n","    val_elastic_losses = []\n","    val_total_losses = []\n","    faust_losses = []\n","    #metrics = []\n","    # Training single Epoch\n","    model.train()\n","    for i, data in enumerate(train_loader):\n","        shapeA, shapeB = data\n","        *shapeA, nameA, gtbasisA = shapeA\n","        *shapeB, nameB, gtbasisB = shapeB\n","        shapeA, shapeB = [x.to(device) for x in shapeA], [x.to(device) for x in shapeB]\n","        gtbasisA, gtbasisB = gtbasisA.to(device).to(torch.float32), gtbasisB.to(device).to(torch.float32)\n","        optim.zero_grad()\n","\n","        # Obtaining predicted basis\n","        #basisA, basisB = model(shapeA, shapeB)\n","        predA, predB = model(shapeA, shapeB)\n","        basisA, basisB = predA[:, :, :n_feat], predB[:, :, :n_feat]\n","        elasticA, elasticB = predA[:, :, n_feat:], predB[:, :, n_feat:]\n","\n","        # Computing optimal transformation\n","        pseudo_inv_A = torch.pinverse(basisA)\n","        C_opt = torch.matmul(pseudo_inv_A, basisB)\n","        opt_A = torch.matmul(basisA, C_opt)\n","\n","        # SoftMap\n","        dist_matrix = torch.cdist(opt_A, basisB)\n","        s_max = torch.nn.Softmax(dim=1)\n","        s_max_matrix = s_max(-dist_matrix)\n","\n","        # Basis Loss\n","        vertsB = shapeB[0] #get vertsB out\n","        eucl_loss = torch.sum(torch.square(torch.matmul(s_max_matrix, vertsB) - vertsB))\n","\n","        # Elastic Loss\n","        elastic_loss = l1_loss(elasticA, gtbasisA) + l1_loss(elasticB, gtbasisB)\n","        #metric = torch.mean(torch.square(s_max_matrix - torch.identity(s_max_matrix.shape[0]))) # measuring how well we are approximating the ground-truth correspondence\n","        # Back Prop\n","        total_loss = eucl_loss + 0.2 * elastic_loss\n","        total_loss.backward()\n","        #eucl_loss.backward()\n","        optim.step()\n","        train_losses.append(eucl_loss.detach().item())\n","        train_elastic_losses.append(elastic_loss.detach().item())\n","        train_total_losses.append(total_loss.detach().item())\n","        #metrics.append(metric.detach().item())\n","\n","    model.eval()\n","    with torch.no_grad():\n","      for i, data in enumerate(test_loader):\n","          shapeA, shapeB = data\n","          *shapeA, nameA, gtbasisA = shapeA\n","          *shapeB, nameB, gtbasisB = shapeB\n","          shapeA, shapeB = [x.to(device) for x in shapeA], [x.to(device) for x in shapeB]\n","          gtbasisA, gtbasisB = gtbasisA.to(device).to(torch.float32), gtbasisB.to(device).to(torch.float32)\n","\n","          #basisA, basisB = model(shapeA, shapeB)\n","          predA, predB = model(shapeA, shapeB)\n","          basisA, basisB = predA[:, :, :n_feat], predB[:, :, :n_feat]\n","          elasticA, elasticB = predA[:, :, n_feat:], predB[:, :, n_feat:]\n","\n","          pseudo_inv_A = torch.pinverse(basisA)\n","          C_opt = torch.matmul(pseudo_inv_A, basisB)\n","          opt_A = torch.matmul(basisA, C_opt)\n","          # SoftMap\n","          dist_matrix = torch.cdist(opt_A, basisB)\n","          s_max = torch.nn.Softmax(dim=1)\n","          s_max_matrix = s_max(-dist_matrix)\n","\n","          # Basis Loss\n","          vertsB = shapeB[0]\n","          val_eucl_loss = torch.sum(torch.square(torch.matmul(s_max_matrix, vertsB) - vertsB))\n","          val_elastic_loss = l1_loss(elasticA, gtbasisA) + l1_loss(elasticB, gtbasisB)\n","          val_total_loss = val_eucl_loss + 0.2 * val_elastic_loss\n","          val_losses.append(val_eucl_loss.detach().item())\n","          val_elastic_losses.append(val_elastic_loss.detach().item())\n","          val_total_losses.append(val_total_loss.detach().item())\n","\n","    ave_train_loss = sum(train_losses) / len(train_losses)\n","    ave_train_elastic_loss = sum(train_elastic_losses) / len(train_elastic_losses)\n","    ave_train_total_loss = sum(train_total_losses) / len(train_total_losses)\n","    ave_val_loss = sum(val_losses) / len(val_losses)\n","    ave_val_elastic_loss = sum(val_elastic_losses) / len(val_elastic_losses)\n","    ave_val_total_loss = sum(val_total_losses) / len(val_total_losses)\n","    #ave_metric = sum(metrics) / len(metrics)\n","    print()\n","    print(f\"training eucl loss for epoch{epoch}:\", ave_train_loss)\n","    print(f\"training elastic loss for epoch{epoch}:\", ave_train_elastic_loss)\n","    print(f\"training total loss for epoch{epoch}:\", ave_train_total_loss)\n","    print(f\"validation eucl loss for epoch{epoch}:\", ave_val_loss)\n","    print(f\"validation elastic loss for epoch{epoch}:\", ave_val_elastic_loss)\n","    print(f\"validation total loss for epoch{epoch}:\", ave_val_total_loss)\n","    #print(f\"training metric for epoch{epoch}:\", ave_metric)\n","    torch.save(model.state_dict(), os.path.join(model_save_dir, f\"train3_k_{n_feat}_epoch_{epoch}\"))\n","    epoch_train_loss.append(ave_train_loss)\n","    epoch_val_loss.append(ave_val_loss)\n","np.save(f\"/content/drive/MyDrive/P3DCV/losses/train/diffusionnet_k_{n_feat}.npy\", np.array(epoch_train_loss))\n","np.save(f\"/content/drive/MyDrive/P3DCV/losses/val/difffmap_k_{n_feat}.npy\", np.array(epoch_val_loss))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CEPJ_LeoEbfE"},"source":["**visualization train and loss curves**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"elapsed":949,"status":"ok","timestamp":1702570390115,"user":{"displayName":"Rui Xiao","userId":"00023545256536816003"},"user_tz":-60},"id":"ejrNFbunEhIh","outputId":"d27402c0-d64f-4dfe-e44c-70b97bebe18f"},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAjsAAAHHCAYAAABZbpmkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABzvUlEQVR4nO3deVhU5dsH8O/MMOyboGyKiKYCikuuqKkpgkrmQqm9aKilZViZP1vMFcwsWyzLpcW0UistNTUVcS/FfUnFTJNEkyUzGBWBYea8f9AcGZiBAWbmwPj9XBeXznOeOee5Z4C5ebYjEwRBABEREZGNkkvdACIiIiJLYrJDRERENo3JDhEREdk0JjtERERk05jsEBERkU1jskNEREQ2jckOERER2TQmO0RERGTTmOwQERGRTWOyQ2QGY8aMQZMmTaRuRrX07t0bvXv3tvp1Db1mMpkMc+bMqfS5c+bMgUwmM2t79u7dC5lMhr1795r1vEQkPSY7ZNNkMplJX/yAM+7EiROQyWSYMWOG0ToXL16ETCbDlClTrNiy6lmyZAlWrlwpdTP09O7dG61bt5a6GSbRaDRYsWIFevfuDS8vLzg4OKBJkyYYO3Ysjh07JnXziAyyk7oBRJb09ddf6z3+6quvkJKSUq48NDS0Rtf57LPPoNVqa3SO2urBBx9ESEgIvvnmG7zxxhsG66xZswYAMGrUqBpd6+7du7Czs+yvpSVLlqB+/foYM2aMXnnPnj1x9+5d2NvbW/T6ddndu3cxbNgwbN++HT179sTrr78OLy8v/Pnnn1i7di2+/PJLZGRkoFGjRlI3lUgPkx2yaWU/fA8dOoSUlJRKP5Tz8/Ph7Oxs8nWUSmW12ldXxMXFYebMmTh06BC6du1a7vg333yDkJAQPPjggzW6jqOjY42eXxNyuVzS69cFL7/8MrZv346FCxdi8uTJesdmz56NhQsXmuU6Wq0WRUVFfD/IbDiMRfc93RDC8ePH0bNnTzg7O+P1118HAPz444+IiYlBQEAAHBwc0KxZM8ydOxcajUbvHGXnn/z555+QyWR499138emnn6JZs2ZwcHBAp06dcPTo0UrbdPPmTUydOhXh4eFwdXWFu7s7BgwYgNOnT+vV080zWbt2LebNm4dGjRrB0dERffv2xaVLl8qdV9cWJycndO7cGT///LNJr1FcXByAez04pR0/fhwXLlwQ65j6mhliaM7OL7/8gk6dOsHR0RHNmjXDJ598YvC5K1asQJ8+feDj4wMHBweEhYVh6dKlenWaNGmCc+fOYd++feIQpm6+krE5O+vWrUOHDh3g5OSE+vXrY9SoUfjrr7/06owZMwaurq7466+/MGTIELi6uqJBgwaYOnWqSXGbasmSJWjVqhUcHBwQEBCAhIQE5Obm6tW5ePEiYmNj4efnB0dHRzRq1AgjR45EXl6eWCclJQU9evSAp6cnXF1d0bJlS/F73phr167hk08+Qb9+/colOgCgUCgwdepUsVfH2Dw2Q/OtZDIZJk2ahNWrV4vxbd68GV5eXhg7dmy5c6hUKjg6OmLq1KliWWFhIWbPno0HHngADg4OCAwMxCuvvILCwsIK46L7A3t2iAD8888/GDBgAEaOHIlRo0bB19cXALBy5Uq4urpiypQpcHV1xe7duzFr1iyoVCq88847lZ53zZo1uHXrFp555hnIZDIsWLAAw4YNw+XLlyvsDbp8+TI2btyIxx9/HMHBwcjOzsYnn3yCXr16IS0tDQEBAXr133rrLcjlckydOhV5eXlYsGAB4uLicPjwYbHO8uXL8cwzz6Bbt26YPHkyLl++jEcffRReXl4IDAysMI7g4GB069YNa9euxcKFC6FQKPRiBID/+7//M8trVtqZM2cQFRWFBg0aYM6cOSguLsbs2bPF96e0pUuXolWrVnj00UdhZ2eHzZs347nnnoNWq0VCQgIA4IMPPsDzzz8PV1dXTJ8+HQAMnktn5cqVGDt2LDp16oT58+cjOzsbH374IQ4cOICTJ0/C09NTrKvRaBAdHY0uXbrg3Xffxc6dO/Hee++hWbNmmDhxYpXiNmTOnDlITExEZGQkJk6ciAsXLmDp0qU4evQoDhw4AKVSiaKiIkRHR6OwsBDPP/88/Pz88Ndff2HLli3Izc2Fh4cHzp07h0ceeQRt2rRBUlISHBwccOnSJRw4cKDC62/btg3FxcUYPXp0jWMxZPfu3Vi7di0mTZqE+vXro3nz5hg6dCjWr1+PTz75RG94cePGjSgsLMTIkSMBlPQEPfroo/jll18wYcIEhIaG4syZM1i4cCF+//13bNy40SJtpjpEILqPJCQkCGW/7Xv16iUAEJYtW1aufn5+frmyZ555RnB2dhYKCgrEsvj4eCEoKEh8nJ6eLgAQvL29hZs3b4rlP/74owBA2Lx5c4XtLCgoEDQajV5Zenq64ODgICQlJYlle/bsEQAIoaGhQmFhoVj+4YcfCgCEM2fOCIIgCEVFRYKPj4/Qrl07vXqffvqpAEDo1atXhe0RBEFYvHixAEBITk4WyzQajdCwYUMhIiJCLKvuayYIggBAmD17tvh4yJAhgqOjo3DlyhWxLC0tTVAoFOXeR0PXjY6OFpo2bapX1qpVK4Px6l7LPXv2CIJw7zVr3bq1cPfuXbHeli1bBADCrFmz9GIBoPfeCIIgtG/fXujQoUO5a5XVq1cvoVWrVkaP5+TkCPb29kJUVJTe98XHH38sABC++OILQRAE4eTJkwIAYd26dUbPtXDhQgGA8Pfff1fartJeeuklAYBw8uRJk+oben8FQRBmz55d7r0DIMjlcuHcuXN65cnJyQZ/XgYOHKj3vn799deCXC4Xfv75Z716y5YtEwAIBw4cMKnNZLs4jEUEwMHBwWB3uZOTk/j/W7du4caNG3jooYeQn5+P3377rdLzjhgxAvXq1RMfP/TQQwBKem4qa49cXvLjqdFo8M8//4jDDSdOnChXf+zYsXp/+Za9zrFjx5CTk4Nnn31Wr96YMWPg4eFRaRy6WJRKpd5Q1r59+/DXX3+JQ1hAzV8zHY1Gg+TkZAwZMgSNGzcWy0NDQxEdHV2ufunr5uXl4caNG+jVqxcuX76sN4RjKt1r9txzz+nNHYmJiUFISAh++umncs959tln9R4/9NBDlb7Xpti5cyeKioowefJk8fsCAMaPHw93d3exLbr3Mjk5Gfn5+QbPpeuN+vHHH6s0qV6lUgEA3NzcqhNCpXr16oWwsDC9sj59+qB+/fr47rvvxLJ///0XKSkpGDFihFi2bt06hIaGIiQkBDdu3BC/+vTpAwDYs2ePRdpMdQeTHSIADRs2NLgK59y5cxg6dCg8PDzg7u6OBg0aiJObTfkALf0hDUBMfP79998Kn6fVarFw4UI0b94cDg4OqF+/Pho0aIBff/3V4HUru86VK1cAAM2bN9erp1Qq0bRp00rjAABvb29ER0djw4YNKCgoAFAyhGVnZ4fhw4eL9Wr6mun8/fffuHv3brk2A0DLli3LlR04cACRkZFwcXGBp6cnGjRoIM5DqU6yo3vNDF0rJCREPK7j6OiIBg0a6JXVq1ev0ve6Jm2xt7dH06ZNxePBwcGYMmUKPv/8c9SvXx/R0dFYvHixXvwjRoxA9+7d8fTTT8PX1xcjR47E2rVrK0183N3dAZQksJYQHBxcrszOzg6xsbH48ccfxbk369evh1qt1kt2Ll68iHPnzqFBgwZ6Xy1atAAA5OTkWKTNVHcw2SGCfq+ATm5uLnr16oXTp08jKSkJmzdvRkpKCt5++20AMOmv4tJzW0oTBKHC57355puYMmUKevbsiVWrViE5ORkpKSlo1aqVwetW9zpVNWrUKKhUKmzZsgVFRUX44YcfxDk1gHles+r4448/0LdvX9y4cQPvv/8+fvrpJ6SkpOCll16y6HVLM/YeWNt7772HX3/9Fa+//jru3r2LF154Aa1atcK1a9cAlHyv79+/Hzt37sTo0aPx66+/YsSIEejXr1+Fk6lDQkIAlMyjMoWxTR+NXcPQzyAAjBw5Erdu3cK2bdsAAGvXrkVISAjatm0r1tFqtQgPD0dKSorBr+eee86kNpPt4gRlIiP27t2Lf/75B+vXr0fPnj3F8vT0dItf+/vvv8fDDz+M5cuX65Xn5uaifv36VT5fUFAQgJK/gHVd+wCgVquRnp6u98FRkUcffRRubm5Ys2YNlEol/v33X70hLHO+Zg0aNICTkxMuXrxY7tiFCxf0Hm/evBmFhYXYtGmTXi+XoeELU3de1r1mFy5c0HvNdGW649ZQui2le+KKioqQnp6OyMhIvfrh4eEIDw/HjBkzcPDgQXTv3h3Lli0T90mSy+Xo27cv+vbti/fffx9vvvkmpk+fjj179pQ7l86AAQOgUCiwatUqkyYp16tXr9xKMQDlesQq07NnT/j7++O7775Djx49sHv3bnFyuU6zZs1w+vRp9O3b1+w7a5NtYM8OkRG6v9RL944UFRVhyZIlVrl22V6ZdevWlVvybKqOHTuiQYMGWLZsGYqKisTylStXGvxAMsbJyQlDhw7F1q1bsXTpUri4uGDw4MF67QbM85opFApER0dj48aNyMjIEMvPnz+P5OTkcnXLXjcvLw8rVqwod14XFxeTYu7YsSN8fHywbNkyveXL27Ztw/nz5xETE1PVkKotMjIS9vb2WLRokV6My5cvR15entgWlUqF4uJiveeGh4dDLpeLMdy8ebPc+du1awcAFS7TDgwMxPjx47Fjxw589NFH5Y5rtVq89957Yg9Ss2bNkJeXh19//VWsk5mZiQ0bNpgYdQm5XI7HHnsMmzdvxtdff43i4mK9ISwAGD58OP766y989tln5Z5/9+5d3Llzp0rXJNvDnh0iI7p164Z69eohPj4eL7zwAmQyGb7++muzDw0Z8sgjjyApKQljx45Ft27dcObMGaxevdrk+TVlKZVKvPHGG3jmmWfQp08fjBgxAunp6VixYkWVzzlq1Ch89dVXSE5ORlxcHFxcXMRj5n7NEhMTsX37djz00EN47rnnUFxcjI8++gitWrXS+xCNioqCvb09Bg0ahGeeeQa3b9/GZ599Bh8fH2RmZuqds0OHDli6dCneeOMNPPDAA/Dx8SnXcwOUvGZvv/02xo4di169euGJJ54Ql543adJEHCIzl7///tvgDtXBwcGIi4vDtGnTkJiYiP79++PRRx/FhQsXsGTJEnTq1EmcE7V7925MmjQJjz/+OFq0aIHi4mJ8/fXXUCgUiI2NBQAkJSVh//79iImJQVBQEHJycrBkyRI0atQIPXr0qLCN7733Hv744w+88MILWL9+PR555BHUq1cPGRkZWLduHX777TdxOfjIkSPx6quvYujQoXjhhReQn5+PpUuXokWLFgYn2VdkxIgR+OijjzB79myEh4eX2/F89OjRWLt2LZ599lns2bMH3bt3h0ajwW+//Ya1a9ciOTkZHTt2rNI1ycZItg6MSALGlp4bW/Z74MABoWvXroKTk5MQEBAgvPLKK+JyWN0SZUEwvvT8nXfeKXdOlFlebUhBQYHwv//9T/D39xecnJyE7t27C6mpqUKvXr30lk3rlkuXXWqsu/6KFSv0ypcsWSIEBwcLDg4OQseOHYX9+/eXO2dliouLBX9/fwGAsHXr1nLHq/uaCYLh12bfvn1Chw4dBHt7e6Fp06bCsmXLDC5f3rRpk9CmTRvB0dFRaNKkifD2228LX3zxhQBASE9PF+tlZWUJMTExgpubm96y+7JLz3W+++47oX379oKDg4Pg5eUlxMXFCdeuXdOrEx8fL7i4uJR7LQy10xDd9geGvvr27SvW+/jjj4WQkBBBqVQKvr6+wsSJE4V///1XPH758mVh3LhxQrNmzQRHR0fBy8tLePjhh4WdO3eKdXbt2iUMHjxYCAgIEOzt7YWAgADhiSeeEH7//fdK2ykIJe//559/Ljz00EOCh4eHoFQqhaCgIGHs2LHllqXv2LFDaN26tWBvby+0bNlSWLVqldGl5wkJCUavqdVqhcDAQAGA8MYbbxisU1RUJLz99ttCq1atBAcHB6FevXpChw4dhMTERCEvL8+k2Mh2yQTBCn+mEhEREUmEc3aIiIjIpjHZISIiIpvGZIeIiIhsGpMdIiIismlMdoiIiMimMdkhIiIim8ZNBVGy8+f169fh5ubGrcaJiIjqCEEQcOvWLQQEBEAur6D/RspNflQqlfDiiy8KjRs3FhwdHYWIiAjhyJEj4nGtVivMnDlT8PPzExwdHYW+ffuW2/jqn3/+Ef7v//5PcHNzEzw8PIRx48YJt27dqlI7rl69anRDL37xi1/84he/+FW7v65evVrh57ykPTtPP/00zp49i6+//hoBAQFYtWoVIiMjkZaWhoYNG2LBggVYtGgRvvzySwQHB2PmzJmIjo5GWloaHB0dAQBxcXHIzMxESkoK1Go1xo4diwkTJmDNmjUmt8PNzQ0AcPXqVbi7u1c5DrVajR07diAqKgpKpbLKz68r7oc4GaNtYIy2gTHaBkvGqFKpEBgYKH6OGyNZsnP37l388MMP+PHHH8W7I8+ZMwebN2/G0qVLMXfuXHzwwQeYMWOGeKPBr776Cr6+vti4cSNGjhyJ8+fPY/v27Th69Kh435OPPvoIAwcOxLvvvouAgACT2qIbunJ3d692suPs7Ax3d3eb/WYF7o84GaNtYIy2gTHaBmvEWNkUFMmSneLiYmg0GrGHRsfJyQm//PIL0tPTkZWVhcjISPGYh4cHunTpgtTUVIwcORKpqanw9PTUu8FbZGQk5HI5Dh8+jKFDhxq8dmFhod7dfVUqFYCSN0StVlc5Ft1zqvPcuuR+iJMx2gbGaBsYo22wZIymnlOyZMfNzQ0RERGYO3cuQkND4evri2+++Qapqal44IEHkJWVBQDw9fXVe56vr694LCsrCz4+PnrH7ezs4OXlJdYxZP78+UhMTCxXvmPHDjg7O1c7ppSUlGo/ty65H+JkjLaBMdoGxmgbLBFjfn6+SfUknbPz9ddfY9y4cWjYsCEUCgUefPBBPPHEEzh+/LhFrztt2jRMmTJFfKwb84uKiqr2MFZKSgr69etns92QwP0RJ2O0DYzRNjBG22DJGHUjM5WRNNlp1qwZ9u3bhzt37kClUsHf3x8jRoxA06ZN4efnBwDIzs6Gv7+/+Jzs7Gy0a9cOAODn54ecnBy9cxYXF+PmzZvi8w1xcHCAg4NDuXKlUlmjN6Kmz68r7oc4GaNtYIzWo9FozD5ModFoYGdnB41GU/Gy4jqMMVZMqVRCoVBUeNwUtWKfHRcXF7i4uODff/9FcnIyFixYgODgYPj5+WHXrl1icqNSqXD48GFMnDgRABAREYHc3FwcP34cHTp0AADs3r0bWq0WXbp0kSocIqL7hiAIyMrKQm5urkXO7efnh6tXr9rsHmiMsXKenp7w8/Or0esjabKTnJwMQRDQsmVLXLp0CS+//DJCQkIwduxYyGQyTJ48GW+88QaaN28uLj0PCAjAkCFDAAChoaHo378/xo8fj2XLlkGtVmPSpEkYOXKkySuxiIio+nSJjo+PD5ydnc36ga3VanH79m24urrabK8HYzROEATk5+eLIzilR3mqStJkJy8vD9OmTcO1a9fg5eWF2NhYzJs3T+yWeuWVV3Dnzh1MmDABubm56NGjB7Zv3663gmv16tWYNGkS+vbtC7lcjtjYWCxatEiqkIiI7hsajUZMdLy9vc1+fq1Wi6KiIjg6Otp0IsAYjXNycgIA5OTkwMfHp8IhrYpImuwMHz4cw4cPN3pcJpMhKSkJSUlJRut4eXlVaQNBIiIyD90cnZqsYiWqjO77S61WVzvZsc00koiIrMZW55pQ7WCO769aMUHZFmm0Ao6k30TOrQL4uDmic7AXFHL+QiAiIrI2JjsWsP1sJhI3pyEzr0As8/dwxOxBYejfuvoTrIiIqPZq0qQJJk+ejMmTJ0vdFCqDw1hmtv1sJiauOqGX6ABAVl4BJq46ge1nMyVqGRFR7aXRCkj94x/8eOovpP7xDzRawWLXkslkFX7NmTOnWuc9evQoJkyYUKO29e7dm8mSBbBnx4w0WgGJm9Ng6EdUACADkLg5Df3C/DikRUT0H2O94TNjQtGtsfknP2dm3vuj87vvvsOsWbNw4cIFsczV1VX8vyAI4qZ4lWnQoIF5G0pmw54dMzp25d9yPTqlCQAy8wpwJP2m9RpFRFSLVdQbnrDmJHZd+Mfs1/Tz8xO/PDw8IJPJxMe//fYb3NzcsG3bNnTo0AEODg745Zdf8Mcff2Dw4MHw9fWFq6srOnXqhJ07d+qdt0mTJvjggw/ExzKZDJ9//jmGDh0KZ2dnNG/eHJs2bapR23/44Qe0atUKDg4OaNKkCd577z2940uWLEHz5s3h6OgIX19fPPbYY+Kx77//HuHh4XBycoK3tzciIyNx586dGrWnrmCyY0Y5tworrwQg55bxhIiIqC4TBAH5RcUmfd0qUGP2pnNGe8MB4O2dl3GrQG3S+QTBfENfr732Gt566y2cP38ebdq0we3btzFw4EDs2rULJ0+eRP/+/TFo0CBkZGRUeJ7ExEQMHz4cv/76KwYOHIi4uDjcvFm9P3iPHz+O4cOHY+TIkThz5gzmzJmDmTNnYuXKlQCAY8eO4YUXXkBSUhIuXLiA7du3o2fPngBKerOeeOIJjBs3DufPn8fevXsxbNgws75mtRmHsczIx638/bYM13OsvBIRUR10V61B2Kxks5xLAJBzqwhtk3ZWWhcA0pKi4Wxvno+1pKQk9OvXT3zs5eWFtm3bio/nzp2LDRs2YNOmTZg0aZLR84wZMwZPPPEEAODNN9/EokWLcOTIEfTv37/KbXr//ffRt29fzJw5EwDQokULpKWl4Z133sGYMWOQkZEBFxcXPPLII3Bzc0NQUBDat28PoCTZKS4uxrBhwxAUFAQACA8Pr3Ib6ir27JhRx6B68PdwhLHZODKUjEN3DvayZrOIiKiKOnbsqPf49u3bmDp1KkJDQ+Hp6QlXV1ecP3++0p6dNm3aiP93cXGBu7t7uRtYm+r8+fPo3r27Xln37t1x8eJFaDQa9OvXD0FBQWjatClGjx6N1atXIz8/HwDQtm1b9O3bF+Hh4Xj88cfx2Wef4d9//61WO+oi9uyYkUIuw+xBYZi46gRkgF7XrC4Bmj0ojJOTichmOSkVSEuKNqnukfSbGLPiaKX1vojvgK7N6pt0bXNxcXHRezx16lSkpKTg3XffxQMPPAAnJyc89thjKCoqqvA8Ze/KLZPJoNVqzdbO0tzc3HDixAns3bsXO3bswKxZszBnzhwcPXoUnp6eSElJwcGDB7Fjxw589NFHmD59Og4fPozg4GCLtKc2Yc+OmfVv7Y+lox6En4f+UJWfhyOWjnqQ++wQkU2TyWRwtrcz6euh5g0q7Q33dbPHQ80bmHQ+S+7kfODAAYwZMwZDhw5FeHg4/Pz88Oeff1rseoaEhobiwIED5drVokUL8TYKdnZ2iIyMxIIFC/Drr7/izz//xO7duwGUvDfdu3dHYmIiTp48CXt7e2zYsMGqMUiFPTsW0L+1P/qF+WHYkgM4fS0PE3s1w9ToluzRISIqxZTe8Fcim9aK353NmzfH+vXrMWjQIMhkMsycOdNiPTR///03Tp06pVfm7++P//3vf+jUqRPmzp2LESNGIDU1FR9//DGWLFkCANiyZQsuX76Mnj17ol69eti6dSu0Wi1atmyJw4cPY9euXYiKioKPjw8OHz6Mv//+G6GhoRaJobZhz46FKOQyNPhvInKQt3Ot+GElIqptKuoNX/x/7dG3pfnvpl4d77//PurVq4du3bph0KBBiI6OxoMPPmiRa61Zswbt27fX+/rss8/w4IMPYu3atfj222/RunVrzJo1C0lJSRgzZgwAwNPTE+vXr0efPn0QGhqKZcuW4ZtvvkGrVq3g7u6O/fv3Y+DAgWjRogVmzJiB9957DwMGDLBIDLUNe3YsyN6uJMFRayyT/RMR2QJdb3jZ+wnKIEClUln02mPGjBGTBaBkB2NDy7GbNGkiDgfpJCQk6D0uO6xl6Dy5ubkVtmfv3r0VHo+NjUVsbKzBYz169DD6/NDQUGzfvr3Cc9syJjsWpFSUdJwVae6PfQyIiKpLIZchopl+L47WgreMoPsLh7EsSJfssGeHiIhIOkx2LEhMdoqZ7BAREUmFyY4F2Ss4Z4eIiEhqTHYsiHN2iIiIpMdkx4KUdpyzQ0REJDUmOxbECcpERETSY7JjQZyzQ0REJD0mOxYkztkp5pwdIiIiqTDZsSAOYxER2a7evXtj8uTJ4uMmTZrggw8+qPA5MpkMGzdurPG1zXWe+wWTHQviBGUiIhNpNUD6z8CZ70v+1WosdqlBgwahf//+Bo/9/PPPkMlk+PXXX6t83qNHj2LChAk1bZ6eOXPmoF27duXKMzMzLX5fq5UrV8LT09Oi17AW3i7Cgjhnh4jIBGmbgO2vAqrr98rcA4Dot4CGvcx+uaeeegqxsbG4du0aGjVqpHdsxYoV6NixI9q0aVPl8zZo0MBcTayUn5+f1a5lC9izY0HcZ4eIqBJpm4C1T+onOgCgyoRsXTyUl7aZ/ZKPPPIIGjRogJUrV+qV3759G+vWrcNTTz2Ff/75B0888QQaNmwIZ2dnhIeH45tvvqnwvGWHsS5evIiePXvC0dERYWFhSElJKfec1157DR07doSrqyuaNm2KmTNnQq1WAyjpWUlMTMTp06chk8kgk8nENpcdxjpz5gz69OkDJycneHt7Y8KECbh9+7Z4fMyYMRgyZAjeffdd+Pv7w9vbGwkJCeK1qiMjIwODBw+Gq6sr3N3dMXz4cGRnZ4vHT58+jYcffhgeHh5o3LgxOnXqhGPHjgEArly5gkGDBqFevXpwcXFBq1atsHXr1mq3pTLs2bEg3i6CiO47ggCo802rq9UA214BYOgPQgGADE575wBhAwA7ZeXnUzoDMlml1ezs7PDkk09i5cqVmD59OmT/PWfdunXQaDR44okncPv2bXTo0AGvvvoq3N3d8dNPP2H06NFo1qwZOnfuXHloWi2GDRsGX19fHD58GHl5eXrze3Tc3NywePFiNG/eHOfOncP48ePh5uaGV155BSNGjMDZs2exfft27Ny5EwDg4eFR7hx37txBdHQ0IiIicPToUeTk5ODpp5/GpEmT9BK6PXv2wN/fH3v27MGlS5cwYsQItGvXDuPHj680HkPx6RKdffv2obi4GAkJCRgxYoR45/W4uDi0b98eixcvxt27d3Hp0iUolSXvY0JCAoqKirB//364uLggLS0Nrq6uVW6HqZjsWBAnKBPRfUedD7wZYJZTySBAdjsLWBBk2hNevw7Yu5hUddy4cXjnnXewb98+9O7dG0DJEFZsbCw8PDzg4eGBqVOnivWff/55JCcnY+3atSYlOzt37sRvv/2G5ORkBASUvB5vvvlmuXk206dPh0qlgru7O5o2bYqpU6fi22+/xSuvvAInJye4urrCzs6uwmGrNWvWoKCgAF999RVcXEri//jjjzFo0CC8/fbb8PX1BQDUq1cPH3/8MRQKBUJCQhATE4Ndu3ZVK9nZtWsXzpw5g/T0dAQGBgIAvvrqK7Rq1QpHjx5Fp06dkJGRgZdffhkhISFQqVRo37495PKSz8WMjAzExsYiPDwcANC0adMqt6EqOIxlQfZ2nLNDRFQbhYSEoFu3bvjiiy8AAJcuXcLPP/+Mp556CgCg0Wgwd+5chIeHw8vLC66urkhOTkZGRoZJ5z9//jwCAwPFRAcAIiIiytX77rvvEB0djYCAALi6umLGjBkmX6P0tdq2bSsmOgDQvXt3aLVaXLhwQSxr1aoVFAqF+Njf3x85OTlVulbpawYGBoqJDgCEhYXB09MT58+fBwBMmTIFTz/9NKKiorBw4UL88ccfYt0XXngBb7zxBrp3747Zs2dXa0J4VUia7Gg0GsycORPBwcFwcnJCs2bNMHfuXAjCvS5NQRAwa9Ys+Pv7w8nJCZGRkbh48aLeeW7evIm4uDi4u7vD09MTTz31lN5YpVTs5JyzQ0T3GaVzSQ+LKV9x35t0Su0Ta007n9K5Sk196qmn8MMPP+DWrVtYsWIFmjVrhl69SiZEv/POO/jwww/x6quvYs+ePTh16hSio6NRVFRU5ZfEmNTUVIwePRr9+vXDpk2bcPLkSUyfPt2s1yhNN4SkI5PJoNVa7o/xOXPm4Ny5cxg4cCB+/vlntG7dGhs2bAAAPP3007h8+TJGjx6NM2fOoGPHjvjoo48s1hZJk523334bS5cuxccff4zz58/j7bffxoIFC/QCXrBgARYtWoRly5bh8OHDcHFxQXR0NAoKCsQ6cXFxOHfuHFJSUrBlyxbs37/f7Mv/qoPDWER035HJSoaSTPlq1qdk1RUMz7MRIIPW1b+kninnM2G+TmnDhw+HXC7HmjVr8NVXX2HcuHHi/J0DBw5g8ODBGDVqFNq2bYumTZvi999/N/ncoaGhuHr1KjIzM8WyQ4cO6dU5ePAggoKCMHXqVHTs2BHNmzfHlStX9OrY29tDo6l4GX5oaChOnz6NO3fuiGUHDhyAXC5Hy5YtTW5zVejiu3r1qliWlpaG3NxchIWFiWUtWrTA5MmTsX79egwdOhQrVqwQjwUGBuLZZ5/F+vXr8b///Q+fffaZRdoKSJzsHDx4EIMHD0ZMTAyaNGmCxx57DFFRUThy5AiAkl6dDz74ADNmzMDgwYPRpk0bfPXVV7h+/bo4C/38+fPYvn07Pv/8c3Tp0gU9evTARx99hG+//RbXr1+v4OqWx2EsIqIKyBVA/7f/e1A2USl5fLf37JJ6FuDq6ooRI0Zg2rRpyMzMxJgxY8RjzZs3R0pKCg4ePIjz58/jmWee0VtpVJnIyEi0aNEC8fHxOH36NH7++WdMnz5dr07z5s2RkZGBH374AX/88QcWLVok9nzoNGnSBOnp6Th16hRu3LiBwsLCcteKi4uDo6Mj4uPjcfbsWezZswfPP/88Ro8eLc7XqS6NRoNTp07pfZ0/fx6RkZEIDw9HXFwcTpw4gSNHjuDJJ59Er1690LFjR9y9exeTJk3C3r17ceXKFRw6dAjHjh1DaGgoAGDy5MlITk5Geno6Tpw4gT179ojHLEHSCcrdunXDp59+it9//x0tWrTA6dOn8csvv+D9998HAKSnpyMrKwuRkZHiczw8PNClSxekpqZi5MiRSE1NhaenJzp27CjWiYyMhFwux+HDhzF06NBy1y0sLNT7hlGpVAAAtVpdrWV4uueUfa5MKElyioq1NVreV1sYi9OWMEbbwBit1wZBEKDVaqs/HBLyCPD4l5AlvwZZqeXngnsAtFFvQt2oNxz/u4YljB07FsuXL8eAAQPg5+cnXuf111/HH3/8gejoaDg7O2P8+PEYPHgw8vLy9NoilGlb6cc//PADxo8fj86dO4vL0gcOHCi+Xo888ghefPFFvPLKKygqKsLAgQMxY8YMJCYmiucYOnQofvjhBzz88MPIzc3F8uXLxaRMdx5HR0ds27YNL730Ejp16gRnZ2cMGzYM7733nngeQRAMtlV3HkO0Wi1u376N9u3b65U3a9YMv//+OzZs2IAXXngBPXv2hFwuR3R0NBYtWgStVguZTIYbN27gySefRHZ2Nry9vTFs2DDMnj0bWq1WXL117do1uLu7Izo6Gu+//77Btmi1WgiCALVarTfnCDD9+18mlJ4gY2VarRavv/46FixYAIVCAY1Gg3nz5mHatGkASnp+unfvjuvXr8Pf31983vDhwyGTyfDdd9/hzTffxJdffqk3CQsAfHx8kJiYiIkTJ5a77pw5c5CYmFiufM2aNXB2rtqYb0X+ugMs+NUObkoBb3S03G6gRERS0K0SCgwMhL29fc1OptXA7q8jkN3JgeDig+KGnS3Wo0N1S1FREa5evYqsrCwUFxfrHcvPz8f//d//IS8vD+7u7kbPIWnPztq1a7F69WqsWbMGrVq1wqlTpzB58mQEBAQgPj7eYtedNm0apkyZIj5WqVQIDAxEVFRUhS+WMWq1GikpKejXr5/eBLA//r6DBb8egNxOiYEDo83SdikZi9OWMEbbwBito6CgAFevXoWrqyscHR1rfkJP/d+TgiDg1q1bcHNzE+fS2BrGWLmCggI4OTmJGzSWphuZqYykyc7LL7+M1157DSNHjgQAhIeH48qVK5g/fz7i4+PFfQWys7P1enays7PFe4X4+fmVWzpXXFyMmzdvGt2XwMHBAQ4ODuXKlUpljX5plH2+s0PJXzpqjWBTv3Br+jrVBYzRNjBGy9JoNJDJZJDL5eL+KeakG9LQXcMWMcbKyeVyyGQyg9/rpn7vS/rK5ufnlwtcoVCIL0xwcDD8/Pywa9cu8bhKpcLhw4fF/QoiIiKQm5uL48ePi3V2794NrVaLLl26WCEK45ScoExERCQ5SXt2Bg0ahHnz5qFx48Zo1aoVTp48iffffx/jxo0DUJIFTp48GW+88QaaN2+O4OBgzJw5EwEBARgyZAiAkuVv/fv3x/jx47Fs2TKo1WpMmjQJI0eO1NvMSQr3lp6XTAyz1S5KIiKi2kzSZOejjz7CzJkz8dxzzyEnJwcBAQF45plnMGvWLLHOK6+8gjt37mDChAnIzc1Fjx49sH37dr1xu9WrV2PSpEno27cv5HI5YmNjsWjRIilC0qNLdoCShEe3FJ2IyJZIuM6F7gPm+P6SNNlxc3PDBx98oHeX2LJkMhmSkpKQlJRktI6XlxfWrFljgRbWjL1esqOFvZ1tjscS0f1JN18iPz8fTk5OEreGbFV+fsmNZWsyN403ArUgpeJeTw7n7RCRrVEoFPD09BQXiTg7O5t1uF6r1aKoqAgFBQU2PXmXMRomCALy8/ORk5MDT0/PcnvsVAWTHQtSyGWQyQBBAIqY7BCRDdKteq3uDSUrIggC7t69CycnJ5ud88gYK+fp6VnhXd9NwWTHgmQyGZQKeckOyrwZKBHZIJlMBn9/f/j4+Jh9N2e1Wo39+/ejZ8+eNruFAGOsmFKprFGPjg6THQuz1yU7xezZISLbpVAozPKhVPacxcXFcHR0tNlEgDFah20OENYiunk7nLNDREQkDSY7FqZbfs45O0RERNJgsmNhpTcWJCIiIutjsmNhur11OIxFREQkDSY7FibO2eEEZSIiIkkw2bEwcRhLy2EsIiIiKTDZsTAx2WHPDhERkSSY7FiYvYJzdoiIiKTEZMfClP/d6ZxLz4mIiKTBZMfCuPSciIhIWkx2LEzJYSwiIiJJMdmxMM7ZISIikhaTHQvT7bNTxNVYREREkmCyY2Gcs0NERCQtJjsWpuTtIoiIiCTFZMfCOGeHiIhIWkx2LEycs8Nkh4iISBJMdizs3u0iOGeHiIhICkx2LIz77BAREUmLyY6F2XOCMhERkaSY7FgY5+wQERFJi8mOhXGfHSIiImkx2bGwexOU2bNDREQkBSY7FsZ9doiIiKTFZMfClHacs0NERCQlJjsWxqXnRERE0pI02WnSpAlkMlm5r4SEBABAQUEBEhIS4O3tDVdXV8TGxiI7O1vvHBkZGYiJiYGzszN8fHzw8ssvo7i4WIpwDOIEZSIiImlJmuwcPXoUmZmZ4ldKSgoA4PHHHwcAvPTSS9i8eTPWrVuHffv24fr16xg2bJj4fI1Gg5iYGBQVFeHgwYP48ssvsXLlSsyaNUuSeAzhnB0iIiJpSZrsNGjQAH5+fuLXli1b0KxZM/Tq1Qt5eXlYvnw53n//ffTp0wcdOnTAihUrcPDgQRw6dAgAsGPHDqSlpWHVqlVo164dBgwYgLlz52Lx4sUoKiqSMjSRrmeniKuxiIiIJFFr5uwUFRVh1apVGDduHGQyGY4fPw61Wo3IyEixTkhICBo3bozU1FQAQGpqKsLDw+Hr6yvWiY6Ohkqlwrlz56wegyG6TQXZs0NERCQNO6kboLNx40bk5uZizJgxAICsrCzY29vD09NTr56vry+ysrLEOqUTHd1x3TFjCgsLUVhYKD5WqVQAALVaDbVaXeW2655j6LkylCQ5RcXaap27NqkoTlvBGG0DY7QNjNE2WDJGU89Za5Kd5cuXY8CAAQgICLD4tebPn4/ExMRy5Tt27ICzs3O1z6ubc1Ta1dsAYIdbd/KxdevWap+7NjEUp61hjLaBMdoGxmgbLBFjfn6+SfVqRbJz5coV7Ny5E+vXrxfL/Pz8UFRUhNzcXL3enezsbPj5+Yl1jhw5oncu3WotXR1Dpk2bhilTpoiPVSoVAgMDERUVBXd39yq3X61WIyUlBf369YNSqdQ7diHrFt49kwqF0gEDB/au8rlrk4ritBWM0TYwRtvAGG2DJWPUjcxUplYkOytWrICPjw9iYmLEsg4dOkCpVGLXrl2IjY0FAFy4cAEZGRmIiIgAAERERGDevHnIycmBj48PgJLM0d3dHWFhYUav5+DgAAcHh3LlSqWyRm+Eoec7OdoDKJmzYyvfyDV9neoCxmgbGKNtYIy2wRIxmno+yZMdrVaLFStWID4+HnZ295rj4eGBp556ClOmTIGXlxfc3d3x/PPPIyIiAl27dgUAREVFISwsDKNHj8aCBQuQlZWFGTNmICEhwWAyIwV77rNDREQkKcmTnZ07dyIjIwPjxo0rd2zhwoWQy+WIjY1FYWEhoqOjsWTJEvG4QqHAli1bMHHiRERERMDFxQXx8fFISkqyZggV4g7KRERE0pI82YmKioIgGO71cHR0xOLFi7F48WKjzw8KCqrVE391S8+LtQK0WgFyuUziFhEREd1fas0+O7ZKaXfvJVZr2btDRERkbUx2LEw3ZwfgvB0iIiIpMNmxMGXpZIe3jCAiIrI6JjsWppDLoJumw0nKRERE1sdkxwrEm4Ey2SEiIrI6JjtWwL12iIiIpMNkxwp0K7I4jEVERGR9THasQLfXThEnKBMREVkdkx0r4C7KRERE0mGyYwWcs0NERCQdJjtWwJ4dIiIi6TDZsQKl3X9zdpjsEBERWR2THSsQe3Y4QZmIiMjqmOxYgZJzdoiIiCTDZMcK7Dlnh4iISDJMdqxA3GeHyQ4REZHVMdmxAq7GIiIikg6THSsQbxfBCcpERERWx2THCripIBERkXSY7FgB5+wQERFJh8mOFejm7BSzZ4eIiMjqmOxYAScoExERSYfJjhXY2zHZISIikgqTHSvgnB0iIiLpMNmxAg5jERERSYfJjhXcuxEoJygTERFZG5MdK+C9sYiIiKTDZMcKOGeHiIhIOkx2rEDJ1VhERESSYbJjBUreLoKIiEgyTHasgHN2iIiIpCN5svPXX39h1KhR8Pb2hpOTE8LDw3Hs2DHxuCAImDVrFvz9/eHk5ITIyEhcvHhR7xw3b95EXFwc3N3d4enpiaeeegq3b9+2dihG6Xp2injXcyIiIquTNNn5999/0b17dyiVSmzbtg1paWl47733UK9ePbHOggULsGjRIixbtgyHDx+Gi4sLoqOjUVBQINaJi4vDuXPnkJKSgi1btmD//v2YMGGCFCEZpJugzJ4dIiIi67OT8uJvv/02AgMDsWLFCrEsODhY/L8gCPjggw8wY8YMDB48GADw1VdfwdfXFxs3bsTIkSNx/vx5bN++HUePHkXHjh0BAB999BEGDhyId999FwEBAdYNyoB7E5Q5Z4eIiMjaJE12Nm3ahOjoaDz++OPYt28fGjZsiOeeew7jx48HAKSnpyMrKwuRkZHiczw8PNClSxekpqZi5MiRSE1Nhaenp5joAEBkZCTkcjkOHz6MoUOHlrtuYWEhCgsLxccqlQoAoFaroVarqxyH7jnGnisXSnp0ioo11Tp/bVFZnLaAMdoGxmgbGKNtsGSMpp5T0mTn8uXLWLp0KaZMmYLXX38dR48exQsvvAB7e3vEx8cjKysLAODr66v3PF9fX/FYVlYWfHx89I7b2dnBy8tLrFPW/PnzkZiYWK58x44dcHZ2rnY8KSkpBsv/UAGAHf7Nu4WtW7dW+/y1hbE4bQljtA2M0TYwRttgiRjz8/NNqidpsqPVatGxY0e8+eabAID27dvj7NmzWLZsGeLj4y123WnTpmHKlCniY5VKhcDAQERFRcHd3b3K51Or1UhJSUG/fv2gVCrLHT91NReLzh2BvZMzBg58qEZtl1JlcdoCxmgbGKNtYIy2wZIx6kZmKiNpsuPv74+wsDC9stDQUPzwww8AAD8/PwBAdnY2/P39xTrZ2dlo166dWCcnJ0fvHMXFxbh586b4/LIcHBzg4OBQrlypVNbojTD2fCcH+5J2aQSb+Gau6etUFzBG28AYbQNjtA2WiNHU80m6Gqt79+64cOGCXtnvv/+OoKAgACWTlf38/LBr1y7xuEqlwuHDhxEREQEAiIiIQG5uLo4fPy7W2b17N7RaLbp06WKFKCpnzx2UiYiIJCNpz85LL72Ebt264c0338Tw4cNx5MgRfPrpp/j0008BADKZDJMnT8Ybb7yB5s2bIzg4GDNnzkRAQACGDBkCoKQnqH///hg/fjyWLVsGtVqNSZMmYeTIkbViJRYA2Ml5bywiIiKpSJrsdOrUCRs2bMC0adOQlJSE4OBgfPDBB4iLixPrvPLKK7hz5w4mTJiA3Nxc9OjRA9u3b4ejo6NYZ/Xq1Zg0aRL69u0LuVyO2NhYLFq0SIqQDFJyB2UiIiLJSJrsAMAjjzyCRx55xOhxmUyGpKQkJCUlGa3j5eWFNWvWWKJ5ZmHPfXaIiIgkI/ntIu4Hup4djVaARsuEh4iIyJqY7FiB7nYRAIeyiIiIrI3JjhXoenYAJjtERETWxmTHCvSTHQ5jERERWROTHStQyGVQyHnncyIiIikw2bES3bydomImO0RERNbEZMdKuNcOERGRNJjsWIm9gnvtEBERSYHJjpWwZ4eIiEgaTHasRGnH+2MRERFJgcmOlYg9O5ygTEREZFVMdqyEc3aIiIikwWTHSjhnh4iISBpMdqxE3GeHyQ4REZFVMdmxEvbsEBERSYPJjpXY2zHZISIikgKTHSu5txqLE5SJiIisicmOlXDODhERkTSY7FgJ5+wQERFJg8mOldgz2SEiIpIEkx0rUXJTQSIiIkkw2bES8d5YvF0EERGRVTHZsRLO2SEiIpIGkx0r4ZwdIiIiaTDZsRLO2SEiIpIGkx0r0SU73GeHiIjIupjsWIlugrKaE5SJiIisismOlXDODhERkTSY7FgJ5+wQERFJg8mOlXDpORERkTQkTXbmzJkDmUym9xUSEiIeLygoQEJCAry9veHq6orY2FhkZ2frnSMjIwMxMTFwdnaGj48PXn75ZRQXF1s7lErpbgTKZIeIiMi67KRuQKtWrbBz507xsZ3dvSa99NJL+Omnn7Bu3Tp4eHhg0qRJGDZsGA4cOAAA0Gg0iImJgZ+fHw4ePIjMzEw8+eSTUCqVePPNN60eS0Xs7TiMRUREJAXJkx07Ozv4+fmVK8/Ly8Py5cuxZs0a9OnTBwCwYsUKhIaG4tChQ+jatSt27NiBtLQ07Ny5E76+vmjXrh3mzp2LV199FXPmzIG9vb21wzGKS8+JiIikIXmyc/HiRQQEBMDR0RERERGYP38+GjdujOPHj0OtViMyMlKsGxISgsaNGyM1NRVdu3ZFamoqwsPD4evrK9aJjo7GxIkTce7cObRv397gNQsLC1FYWCg+VqlUAAC1Wg21Wl3lGHTPqei5cqEkySkq1lTrGrWBKXHWdYzRNjBG28AYbYMlYzT1nJImO126dMHKlSvRsmVLZGZmIjExEQ899BDOnj2LrKws2Nvbw9PTU+85vr6+yMrKAgBkZWXpJTq647pjxsyfPx+JiYnlynfs2AFnZ+dqx5OSkmL02Pl/ZQAUuHEzF1u3bq32NWqDiuK0FYzRNjBG28AYbYMlYszPzzepnqTJzoABA8T/t2nTBl26dEFQUBDWrl0LJycni1132rRpmDJlivhYpVIhMDAQUVFRcHd3r/L51Go1UlJS0K9fPyiVSoN16l3+B8t+Ow5nFzcMHNit2m2Xkilx1nWM0TYwRtvAGG2DJWPUjcxURvJhrNI8PT3RokULXLp0Cf369UNRURFyc3P1eneys7PFOT5+fn44cuSI3jl0q7UMzQPScXBwgIODQ7lypVJZozeiouc7OZTMHyrWCnX+G7qmr1NdwBhtA2O0DYzRNlgiRlPPV6v22bl9+zb++OMP+Pv7o0OHDlAqldi1a5d4/MKFC8jIyEBERAQAICIiAmfOnEFOTo5YJyUlBe7u7ggLC7N6+yvCCcpERETSqFayc/XqVVy7dk18fOTIEUyePBmffvpplc4zdepU7Nu3D3/++ScOHjyIoUOHQqFQ4IknnoCHhweeeuopTJkyBXv27MHx48cxduxYREREoGvXrgCAqKgohIWFYfTo0Th9+jSSk5MxY8YMJCQkGOy5kRL32SEiIpJGtZKd//u//8OePXsAlEwE7tevH44cOYLp06cjKSnJ5PNcu3YNTzzxBFq2bInhw4fD29sbhw4dQoMGDQAACxcuxCOPPILY2Fj07NkTfn5+WL9+vfh8hUKBLVu2QKFQICIiAqNGjcKTTz5ZpTZYiz1vF0FERCSJas3ZOXv2LDp37gwAWLt2LVq3bo0DBw5gx44dePbZZzFr1iyTzvPtt99WeNzR0RGLFy/G4sWLjdYJCgqqE6ubxNtF8K7nREREVlWtnh21Wi0OE+3cuROPPvoogJJ9cDIzM83XOhuitOOcHSIiIilUK9lp1aoVli1bhp9//hkpKSno378/AOD69evw9vY2awNtBefsEBERSaNayc7bb7+NTz75BL1798YTTzyBtm3bAgA2bdokDm+RPqW85KXWCoBGy3k7RERE1lKtOTu9e/fGjRs3oFKpUK9ePbF8woQJNdqB2JbphrGAkt4dhVwhYWuIiIjuH9Xq2bl79y4KCwvFROfKlSv44IMPcOHCBfj4+Ji1gbZCN4wFcN4OERGRNVUr2Rk8eDC++uorAEBubi66dOmC9957D0OGDMHSpUvN2kBboRvGArgii4iIyJqqleycOHECDz30EADg+++/h6+vL65cuYKvvvoKixYtMmsDbYVcLoOdXDdJmXN2iIiIrKVayU5+fj7c3NwAlNwpfNiwYZDL5ejatSuuXLli1gbaEnGvHQ5jERERWU21kp0HHngAGzduxNWrV5GcnIyoqCgAQE5OTrXuGn6/0M3b4ZwdIiIi66lWsjNr1ixMnToVTZo0QefOncUbc+7YsQPt27c3awNtib0de3aIiIisrVpLzx977DH06NEDmZmZ4h47ANC3b18MHTrUbI2zNfduGcE5O0RERNZSrWQHAPz8/ODn5yfe/bxRo0bcULASumSHw1hERETWU61hLK1Wi6SkJHh4eCAoKAhBQUHw9PTE3LlzodXyg9wY3jKCiIjI+qrVszN9+nQsX74cb731Frp37w4A+OWXXzBnzhwUFBRg3rx5Zm2kreBqLCIiIuurVrLz5Zdf4vPPPxfvdg4Abdq0QcOGDfHcc88x2TGCE5SJiIisr1rDWDdv3kRISEi58pCQENy8ebPGjbJV4pwdTlAmIiKymmolO23btsXHH39crvzjjz9GmzZtatwoW8U5O0RERNZXrWGsBQsWICYmBjt37hT32ElNTcXVq1exdetWszbQlnDODhERkfVVq2enV69e+P333zF06FDk5uYiNzcXw4YNw7lz5/D111+bu402w57JDhERkdVVe5+dgICAchORT58+jeXLl+PTTz+tccNs0b19djhnh4iIyFqq1bND1aPUrcYqZs8OERGRtTDZsSJOUCYiIrI+JjtWxDk7RERE1lelOTvDhg2r8Hhubm5N2mLzOGeHiIjI+qqU7Hh4eFR6/Mknn6xRg2wZl54TERFZX5WSnRUrVliqHfcFpd1/c3Y4QZmIiMhqOGfHijhnh4iIyPqY7FgR5+wQERFZH5MdK+KcHSIiIutjsmNF3GeHiIjI+mpNsvPWW29BJpNh8uTJYllBQQESEhLg7e0NV1dXxMbGIjs7W+95GRkZiImJgbOzM3x8fPDyyy+juLjYyq03jb0de3aIiIisrVYkO0ePHsUnn3yCNm3a6JW/9NJL2Lx5M9atW4d9+/bh+vXrenv9aDQaxMTEoKioCAcPHsSXX36JlStXYtasWdYOwSTinJ1iztkhIiKyFsmTndu3byMuLg6fffYZ6tWrJ5bn5eVh+fLleP/999GnTx906NABK1aswMGDB3Ho0CEAwI4dO5CWloZVq1ahXbt2GDBgAObOnYvFixejqKhIqpCM4pwdIiIi65M82UlISEBMTAwiIyP1yo8fPw61Wq1XHhISgsaNGyM1NRUAkJqaivDwcPj6+op1oqOjoVKpcO7cOesEUAWcs0NERGR9VdpU0Ny+/fZbnDhxAkePHi13LCsrC/b29vD09NQr9/X1RVZWllindKKjO647ZkxhYSEKCwvFxyqVCgCgVquhVqurHIfuOZU9V46S4auiYk21riM1U+OsyxijbWCMtoEx2gZLxmjqOSVLdq5evYoXX3wRKSkpcHR0tOq158+fj8TExHLlO3bsgLOzc7XPm5KSUuHxszdlABT4+59/sXXr1mpfR2qVxWkLGKNtYIy2gTHaBkvEmJ+fb1I9yZKd48ePIycnBw8++KBYptFosH//fnz88cdITk5GUVERcnNz9Xp3srOz4efnBwDw8/PDkSNH9M6rW62lq2PItGnTMGXKFPGxSqVCYGAgoqKi4O7uXuVY1Go1UlJS0K9fPyiVSqP13C7ewGcXTsDZ1R0DB0ZU+TpSMzXOuowx2gbGaBsYo22wZIy6kZnKSJbs9O3bF2fOnNErGzt2LEJCQvDqq68iMDAQSqUSu3btQmxsLADgwoULyMjIQERESaIQERGBefPmIScnBz4+PgBKMkd3d3eEhYUZvbaDgwMcHBzKlSuVyhq9EZU939Gh5FixVqjT39Q1fZ3qAsZoGxijbWCMtsESMZp6PsmSHTc3N7Ru3VqvzMXFBd7e3mL5U089hSlTpsDLywvu7u54/vnnERERga5duwIAoqKiEBYWhtGjR2PBggXIysrCjBkzkJCQYDCZkRrvjUVERGR9kk5QrszChQshl8sRGxuLwsJCREdHY8mSJeJxhUKBLVu2YOLEiYiIiICLiwvi4+ORlJQkYauNu7f0nPvsEBERWUutSnb27t2r99jR0RGLFy/G4sWLjT4nKCiozkz25T47RERE1if5Pjv3E3s77rNDRERkbUx2rIjDWERERNbHZMeKxHtjsWeHiIjIapjsWFHpOTuCwN4dIiIia2CyY0W6peeCAGi0THaIiIisgcmOFSn/m6AMcN4OERGRtTDZsSI7+b2Xm/N2iIiIrIPJjhUpFaV7dpjsEBERWQOTHSuSyWRiwsNkh4iIyDqY7FiZuCKrmHN2iIiIrIHJjpVxrx0iIiLrYrJjZbw/FhERkXUx2bEye87ZISIisiomO1amtGPPDhERkTUx2bEycc4OJygTERFZBZMdK+OcHSIiIutismNlnLNDRERkXUx2rIw9O0RERNbFZMfK7u2zwzk7RERE1sBkx8rE1VjF7NkhIiKyBiY7VsY5O0RERNbFZMfKOGeHiIjIupjsWBnn7BAREVkXkx0rY88OERGRdTHZsTJ7u//m7HCCMhERkVUw2bEy9uwQERFZF5MdK+OcHSIiIutismNl7NkhIiKyLiY7VsZ9doiIiKyLyY6VsWeHiIjIupjsWJnudhFFxZyzQ0REZA2SJjtLly5FmzZt4O7uDnd3d0RERGDbtm3i8YKCAiQkJMDb2xuurq6IjY1Fdna23jkyMjIQExMDZ2dn+Pj44OWXX0ZxcbG1QzEZe3aIiIisS9Jkp1GjRnjrrbdw/PhxHDt2DH369MHgwYNx7tw5AMBLL72EzZs3Y926ddi3bx+uX7+OYcOGic/XaDSIiYlBUVERDh48iC+//BIrV67ErFmzpAqpUpyzQ0REZF12Ul580KBBeo/nzZuHpUuX4tChQ2jUqBGWL1+ONWvWoE+fPgCAFStWIDQ0FIcOHULXrl2xY8cOpKWlYefOnfD19UW7du0wd+5cvPrqq5gzZw7s7e2lCKtC7NkhIiKyLkmTndI0Gg3WrVuHO3fuICIiAsePH4darUZkZKRYJyQkBI0bN0Zqaiq6du2K1NRUhIeHw9fXV6wTHR2NiRMn4ty5c2jfvr3BaxUWFqKwsFB8rFKpAABqtRpqtbrKbdc9x5TnylEyV6dQranWtaRUlTjrKsZoGxijbWCMtsGSMZp6TsmTnTNnziAiIgIFBQVwdXXFhg0bEBYWhlOnTsHe3h6enp569X19fZGVlQUAyMrK0kt0dMd1x4yZP38+EhMTy5Xv2LEDzs7O1Y4lJSWl0jrnbsgAKJCV8ze2bt1a7WtJyZQ46zrGaBsYo21gjLbBEjHm5+ebVE/yZKdly5Y4deoU8vLy8P333yM+Ph779u2z6DWnTZuGKVOmiI9VKhUCAwMRFRUFd3f3Kp9PrVYjJSUF/fr1g1KprLCu4lw2vrx4Gu6eXhg4sHOVryWlqsRZVzFG28AYbQNjtA2WjFE3MlMZyZMde3t7PPDAAwCADh064OjRo/jwww8xYsQIFBUVITc3V693Jzs7G35+fgAAPz8/HDlyRO98utVaujqGODg4wMHBoVy5Uqms0RthyvOdHEqOF2uFOvuNXdPXqS5gjLaBMdoGxmgbLBGjqeerdfvsaLVaFBYWokOHDlAqldi1a5d47MKFC8jIyEBERAQAICIiAmfOnEFOTo5YJyUlBe7u7ggLC7N6203Be2MRERFZl6Q9O9OmTcOAAQPQuHFj3Lp1C2vWrMHevXuRnJwMDw8PPPXUU5gyZQq8vLzg7u6O559/HhEREejatSsAICoqCmFhYRg9ejQWLFiArKwszJgxAwkJCQZ7bmoDhaxk6fnNO4VI/eMfdA72gkIuk7hVREREtkvSZCcnJwdPPvkkMjMz4eHhgTZt2iA5ORn9+vUDACxcuBByuRyxsbEoLCxEdHQ0lixZIj5foVBgy5YtmDhxIiIiIuDi4oL4+HgkJSVJFVKFtp/NxPQNZwEA2apCPPHZIfh7OGL2oDD0b+0vceuIiIhsk6TJzvLlyys87ujoiMWLF2Px4sVG6wQFBdWJVU3bz2Zi4qoTKDt4lZVXgImrTmDpqAeZ8BAREVlArZuzY4s0WgGJm9PKJToAxLLEzWnQaDmPh4iIyNyY7FjBkfSbyMwrMHpcAJCZV4Aj6Tet1ygiIqL7BJMdK8i5ZTzRqU49IiIiMh2THSvwcXM0az0iIiIyHZMdK+gc7AV/D0cYW2AuA+Dv4YjOwV7WbBYREdF9gcmOFSjkMsweVLLJYdmER/d49qAw7rdDRERkAUx2rKR/a38sHfUg/Dz0h6r8PBy57JyIiMiCJL831v2kf2t/9AvzQ+ofNzB25VGoNQK+GNMJof5Vv/koERERmYY9O1amkMvQo3kDdAwqmZ9zMiNX2gYRERHZOCY7EunYpB4A4PiVfyVuCRERkW1jsiORB4N0yQ43EiQiIrIkJjsSebBxSbLz5z/5+PtWocStISIisl1MdiTi4aREC19XAMCJDA5lERERWQqTHQl1+G+SMuftEBERWQ6THQl1DOIkZSIiIktjsiOhDv8lO2eu5aFArZG4NURERLaJyY6EgrydUd/VHkUaLc5dz5O6OURERDaJyY6EZDKZuCrr2J8cyiIiIrIEJjsS020ueIzzdoiIiCyCyY7EdCuyTlz5F4IgSNwaIiIi28NkR2KtG7rD3k6Of+4U4c9/8qVuDhERkc1hsmMpWg2Q/jNw5vuSf7WGV1s52CnQpqEHAC5BJyIisgQ7qRtgk9I2AdtfBVTX75W5BwD93wbCHi1XvUNQPRy78i+2nrkOpUIGHzdHdA72gkIus2KjiYiIbBOTHXNL2wSsfRJAmfk3qsyS8uFflUt4ZP/lNLt/+xu7f/sbAODv4YjZg8LQv7W/FRpNRERkuziMZU5aTUmPTtlEB7hXtv01vSGt7WczsWzf5XK1s/IKMHHVCWw/m2mZthIREd0nmOyYkexqqv7QVTkCoPoLuHIQAKDRCkjcnGasJgAgcXMaNFqu0iIiIqouJjvmdDu7SvWOpN9EZl6B0WoCgMy8AhxJv2mGxhEREd2fmOyYk6tvlerl3DKe6JRmaj0iIiIqj8mOGQmBESWrrmBsFZUMcG8IBHUDAPi4OZp0XlPrERERUXlMdsxJrihZXg6gfMLz3+P+b5XUA9A52Av+Ho4VpUbw9yhZhk5ERETVI2myM3/+fHTq1Alubm7w8fHBkCFDcOHCBb06BQUFSEhIgLe3N1xdXREbG4vsbP25MRkZGYiJiYGzszN8fHzw8ssvo7i42Jqh3BP2aMnycvcyS8advcotO1fIZZg9KAyA8b6g2YPCuN8OERFRDUia7Ozbtw8JCQk4dOgQUlJSoFarERUVhTt37oh1XnrpJWzevBnr1q3Dvn37cP36dQwbNkw8rtFoEBMTg6KiIhw8eBBffvklVq5ciVmzZkkRUomwR4HJZ4H4LUDjkiErdBhrcEPB/q39sXTUg/Dz0B+qUshlWPx/D3KfHSIiohqSdFPB7du36z1euXIlfHx8cPz4cfTs2RN5eXlYvnw51qxZgz59+gAAVqxYgdDQUBw6dAhdu3bFjh07kJaWhp07d8LX1xft2rXD3Llz8eqrr2LOnDmwt7eXIrSSoargh4AbjwEZB4G/jhmt2r+1P/qF+eFI+k1c/TcfiZvO4U6RBsW8MSgREVGN1aodlPPy8gAAXl4lc1SOHz8OtVqNyMhIsU5ISAgaN26M1NRUdO3aFampqQgPD4ev772VUNHR0Zg4cSLOnTuH9u3bl7tOYWEhCgsLxccqlQoAoFaroVarq9xu3XMMPtevPZQAhGvHUFxYIM7XMaRjY3d0bOyOazfvYNHuP/DRrt8RHVIf8loyjFVhnDaCMdoGxmgbGKNtsGSMpp6z1iQ7Wq0WkydPRvfu3dG6dWsAQFZWFuzt7eHp6alX19fXF1lZWWKd0omO7rjumCHz589HYmJiufIdO3bA2dm52jGkpKSUK5MJGgyU28Ou6DZ+3rAct5waVXoe/2LAUaHAxZw7mPzZdjRwAtyVQDN3AbUh7zEUp61hjLaBMdoGxmgbLBFjfn6+SfVqTbKTkJCAs2fP4pdffrH4taZNm4YpU6aIj1UqFQIDAxEVFQV3d/cqn0+tViMlJQX9+vWDUqksd1x+8zMg4yB6NnOG0G6gSefce+cUktNysO3avZ4gP3cHzBgYguhWJu7nY2aVxWkLGKNtYIy2gTHaBkvGqBuZqUytSHYmTZqELVu2YP/+/WjU6F7Ph5+fH4qKipCbm6vXu5OdnQ0/Pz+xzpEjR/TOp1utpatTloODAxwcHMqVK5XKGr0RRp8f2BnIOAi7zBNAp7GVnmf72UzsSMspV56tKsTz357G0lHSTlyu6etUFzBG28AYbQNjtA2WiNHU80m6GksQBEyaNAkbNmzA7t27ERwcrHe8Q4cOUCqV2LVrl1h24cIFZGRkICIiAgAQERGBM2fOICfnXnKQkpICd3d3hIWFWSeQygR2Lvn3mvFJyjq6+2VVcCtR3i+LiIioCiTt2UlISMCaNWvw448/ws3NTZxj4+HhAScnJ3h4eOCpp57ClClT4OXlBXd3dzz//POIiIhA165dAQBRUVEICwvD6NGjsWDBAmRlZWHGjBlISEgw2HsjiYYdS/7NOQ8UqABH40NlVblfVkQzbzM3lIiIyPZI2rOzdOlS5OXloXfv3vD39xe/vvvuO7HOwoUL8cgjjyA2NhY9e/aEn58f1q9fLx5XKBTYsmULFAoFIiIiMGrUKDz55JNISkqSIiTD3HwBz8YABOCv4xVW5f2yiIiIzEvSnh3BhH1kHB0dsXjxYixevNhonaCgIGzdutWcTTO/Rp2A3IySoaxmDxutxvtlERERmRfvjWUtjTqV/HvtaIXVKrtfFsD7ZREREVUFkx1rKZ3sVNCjZcr9sh5p448j6Tfx46m/kPrHP5ysTEREVIFasfT8vuAXDijsgbs3gZuXAe9mRqvq7peVuDlNb7Kyi4MCdwo1+PzndHz2c7pY7u/hiNmDwngfLSIiIgOY7FiLnQPg37akZ+fasQqTHUD/flk5twrg4+aIv28V4IVvT5Vblp6VV4CJq05Ivv8OERFRbcRhLGtqpNtvp+J5OzoKuQwRzbwxuF1DdA72wvxtvxmsx/13iIiIjGOyY02N/ttvx8Rkp7Sq7L9DRERE93AYy5p0k5SzfgVOri7ZeyeoW4V3QtcxdV+dA5f+Foe9Ogd7QVEb7hxKREQkISY71nT9BCCTA4IW+PG5kjL3AKD/20DYoxU+1dR9dT7e84f4f05cJiIi4jCW9aRtAtbGlyQ6pakygbVPlhyvgCn775Slm7i8/Wxm1dtLRERkI5jsWINWA2x/Fajo9p7bXyupZ4Qp++8YOTMnLhMR0X2NyY41XDkIqK5XUEEAVH+V1KuAbv8dPw/TbxWhm7i8MOV3bkBIRET3Jc7ZsYbb2WarV3b/nYvZt/HxnkuVPu/jPZfw8Z5LnMdDRET3HfbsWIOrr1nrld5/p/sD9avUFM7jISKi+w2THWsI6lay6srobBsZ4N6wpF4VVXXiMufxEBHR/YbJjjXIFSXLywEYTngEoP9bJu23U1Z1Jy5n5hVg5YF03kyUiIhsHufsWEvYo8Dwr0pWZZWdrGznBAQ8CKT/XDJvx9XX5M0GAeM3Dq3M3J/Oi//nXB4iIrJVTHasKexRICSmZNXV7WzApQGwMxG4fhz4uANQXCpRMXGzQZ3SE5cPXPpbb3NBU/BmokREZKuY7FibXAEEP3TvcethJclOcZkeGd1mg8O/Mjnh0U1c7hzshR9O/IWsvAKDO/sYIqBkGGzOpnNwc1Tixu1C3nKCiIhsApMdKWk1wKHFRg7+l35sf62kN6gK83l083gmrjoBGQxvZWjsilmqQsR9flgs4/AWERFVSqu5N2pRxakY1sAJylIy02aDhlRnA0JDdMNbW3+9jsPpN3H8hgyH029yQjMREZVI2wR80Br48hHgh6dK/v2gdaW3QbIm9uxIydTNBs//9w1TxUy57AaEN24V6k1KNoUupZn0zUmU5DcKfHXxGHt8iIhqA6l7VNI2lUy5KDuGoJuK8dhKyBw90PBmKmRX3IGmPSXp8WGyIyVTNxs88mnJVxUnLQP35vEAgEYr4PNf0qs0l0enbEdOVl4Bnl11Ai9FNkeT+i6c30NE9xdjSYahcsBoXdmVX8onAqYmMGmbyq/wrexzoortq7D8ViawfRoqvO/jD2NhJ2jREQCuLK3W55g5MNmRkm6zQVUmTJpZUypThot3lTP56s7lMUT33IU7L4pl7O0hovuCsSSj9WPA2e/1y53qAZABd28arGunuq6fCBg6h3sAEDVf//d+/j/AujGoqEel3OfEbz+Vb3cl7TMpxooIWsPtq8LiG3NgsiMl3WaDa58ETEo/7mXKet9Auky59LJ2I0lQdffkMYVufs/i/2uPei4OyLlVwB4fIpJOVYZ4TO2pMZpkXAcOLip/3rv/li8zVrei8u/j9ctk8vJtAGD0c8KpnuG2mKN9VVL9xTc1wWRHahVtNmiMwUx5NODkVT47L/vXQFC3cnN56rs44H/rTiNbVfXhLb1m/ffvvfk9JdjjQ1RKVYYRLHluMw3DWPLcBod4TI3bUC+GsT8M8/8BkqeZ1othNMmwsrKfA5UdN5TUSKbU4pvSW7FYEJOd2qD0ZoPnN5XMz6mS/37wSic6gOG/Bv77YVeEPSrO5QGAOY+aZ3gLMDy/hz0+RDA8/GFsGKGi3lpDiYClhijMcQ5zDfGYmqgY68Uw9oehIcZ6MSpLMsh0pi7SMQOZIAi1IEWVlkqlgoeHB/Ly8uDu7l7l56vVamzduhUDBw6EUqmsWWPSfy5Ztmcx/yUXBsZzt6fllBveksvKJy/VVfZc/h6OmBkTWqsSILO+l7VUnYuxGqtNahyjJXo9jA1/GPTfnx2GemuNJQ216i93SzDymlDdFb+lxj07pn5+s2entqnqpOUqMz7vp3//t9Hv5YH47XAy7v77F5zqNcQV17ZI+OY05NCik/w3+CAXOfDEEW0IAKBzmTJtBVs3lU2aMvMK8Nyak3pluiGv0sNstSEJui9VdYjCEqq52sTkFS6A6cMfNe31qNLwRwW9tabOC7E5Rl4Tqp1k8gp6wWQlPye6n0ErYLJT21R50nI1GZn3o3DyQqtSv0xauQdgV5sBcPl9I3zxj1h+U3AFAHjJbotl1wUvJKqfRIq2o8EkSA5tuXJAP2E6mheCZ1edgJeTHC0Kz4rlV13bYuaj4ffHvB8Lz6UolwgYOkdVPvArGnKpRvvE8n/+APbOR3VWm5i0wsVQomJ0+MMMk085/EG2zrk+0H8+4OZfqicT0P8Z/u+P1v5vWXW/HQ5joZYNY+kY+ou2wkzZsnT3zhIf//ddIytVqOu5yYVruSRoU3E3PGp3EAGyex8sxhImQ3WvC15IUj+JR0c8jaDbv4o9TyFdoqGwM5KzV2PyZPHl/Tj1czLaPRQNu+psflXT1R+W6lUwxwe+UUaGFyy9pLXsz8N9MZRDVEbZnwNJPif++yAou5zcYM9sw5JEx0zLzk39/JY02dm/fz/eeecdHD9+HJmZmdiwYQOGDBkiHhcEAbNnz8Znn32G3NxcdO/eHUuXLkXz5s3FOjdv3sTzzz+PzZs3Qy6XIzY2Fh9++CFcXV1NbketTHaACsb8gVqxGsAAY0mQ7mHp8qrU1SVSeXBFvVLJUTa8cT1iNtr0jdMbfgvxKIJix+s1mzxpZDWb0YTJWKJi6BzGJpPyw5qobiibVLg3BFrHGviDwqvkX72k30hdY+WGG1DyT9kezgp7VAz9UWKG9lWUwJjjj8gK1IlkZ9u2bThw4AA6dOiAYcOGlUt23n77bcyfPx9ffvklgoODMXPmTJw5cwZpaWlwdCy559OAAQOQmZmJTz75BGq1GmPHjkWnTp2wZs0ak9tRa5MdQwyu5tB981pw2KuGBEE/ealO3Yp6k1QyV3jiXhKkexX0eqPKPK4Wc0wQZVJDVIcZX+RRneFmg4mAoT90yy2NryDBqKhHxdzDzSbM27PkZ2SdSHZKk8lkesmOIAgICAjA//73P0ydOhUAkJeXB19fX6xcuRIjR47E+fPnERYWhqNHj6Jjx44AgO3bt2PgwIG4du0aAgICTLp2nUp2ANN7Fe4ThpIgowkTzJDwEFEtV0Evhql/GFanF6MaTP78qOrCAKnvmVVKbUh2au0E5fT0dGRlZSEyMlIs8/DwQJcuXZCamoqRI0ciNTUVnp6eYqIDAJGRkZDL5Th8+DCGDh0qRdMtT64ov1yv9F49Ff01IOG8H0sxlNQY60ViokOSKzfXyMAwQpV7ay04RGGuYY6anrtKiUqA8V4Mg8PNDYGoNw331ETOqR1Jg6Hf++asb+NqbbKTlZUFAPD11b9Zpq+vr3gsKysLPj4+esft7Ozg5eUl1jGksLAQhYWF4mOVSgWgJPtUq9VVbqvuOdV5rtk16qr/+IH+kF1NLZUA3YRi/VMAAFmpXxiGhn1KH2OSQEDd/V6oSruNDYGignLT6pY80gz9HHD2En8mhcCIkrqlfk6FwAjIft8GxY7XIbt170NZcG8IbdhQyM+tL1MeAE2/eRBaDCh3HkPnhlwB9Jpuerk5zlGNc2vSf8HZ1J1oHREJRXAPo6+JJnJu+ddUrgA0Wv3fiRot0HwA0CzK8DVL02hLvoDy59CY7w/GWvX5YSGWjNHUc9baZMeS5s+fj8TExHLlO3bsgLOzc7XPm5KSUpNmWZgTABUAO/gHT0L4tdVwUt/7a6pI4Qp7ze3yq65K/WvqL/maJkxVmd9zv6jqh3WN34P//q3u94I521e2XAsZZGLqUL5u2bZU53u4SOEKB829OWB3lV74q14XNPr3cLmfGwAm1b2rrIezjeKQmW6Hkp/F/34mzyWXakHpMjnQ7E14374AR3UuCpSe+Me1JVAoB5p1LF9+WQ5cTjZwHkPnRjXLzXGOKp7bKwJ/XbgDXKjgNUmXw/hrWpGq1rec2v35YR6WiDE/P9+kerU22fHz8wMAZGdnw9//3t4q2dnZaNeunVgnJydH73nFxcW4efOm+HxDpk2bhilTpoiPVSoVAgMDERUVVe05OykpKejXr1/d2JEWAwHtDBSX+stGHhgBzX9/NeGWfveu7i9JvXInLwCC/kTb/7qZhf8+jHSq/GEjM22pe3VUNPm5NiRY5vqwrlnSqhsSKTORuqLvhQre95q1r6Qtmp6vQvBqqtc7WVJXKFe3Su0u+z3s3hCafvMgbzFA7+dDGRiBJroJpWV+bgCYVFcZGIH2cgXao6oM76iuVkfXsd87VWf8d6sld5m3rrr3+VF1loxRNzJTmVqb7AQHB8PPzw+7du0SkxuVSoXDhw9j4sSJAICIiAjk5ubi+PHj6NChAwBg9+7d0Gq16NKli9FzOzg4wMHBoVy5Uqms0RtR0+dblxJ44GH9ovChQKtH9canZUHdoJArgKik8qsGgHJ1DY2Hy4yMwcsMjNfr6gpl6hY5eMK+MBdaoeS2EzoVJUFlExjdsnZTy6uaGFV1ojRQtsfiv8flbhHQELIqvn7l6lbpPTA836H094JJ77s52vdfWxRlJ4PaKQ18n91rd9nvVaPtBsqV2emGM8r+fAAw+HNT1bpmVrd+71QPY7QNlojR1PNJmuzcvn0bly5dEh+np6fj1KlT8PLyQuPGjTF58mS88cYbaN68ubj0PCAgQFyxFRoaiv79+2P8+PFYtmwZ1Go1Jk2ahJEjR5q8EovKMDapTa6AENQDf51ToW1Qj3vj26ZMlK5ooh9gsK6sTF2HoG44mbIKAamJejs558pc4SncLpcEld6vRycL3thUHFGyYSFuVloOVC1hAv7bULHUEnhj5xDrltozKEvwRqJ6NI7KuqJFUandowvaYmZAOPr1ma2/l1CXaAAoV6aws4PGQF2FnR0QOafSpFVvAqaR7wWT33dj5zY26dPUyaAVfZ8Bhr9XjX1vcxInkc2TNNk5duwYHn743l8+uqGl+Ph4rFy5Eq+88gru3LmDCRMmIDc3Fz169MD27dvFPXYAYPXq1Zg0aRL69u0rbiq4aJGBrdrJeipImEz+sDFQt310PDR943CuzIf4qV2ryyVBWfBGkjoOuXAvd9uKBZqRBm9nUba8Hm5hpvJrkxKjLJQkKmVvlWHsHIbqivcWu6vFIYSJ9WUqNZ5ddQKezkrk5gNAQwCA567dAKBX5r9/Hx5t649NpzORmadfrrvn2CFtGHZob6FAG4aI/+5ldkQbhhxNU/hoHdEZclRrrUkN398Ky6tyPSKiMiRNdnr37o2KtvmRyWRISkpCUlKS0TpeXl5V2kCQ6jaFnR1adY/RKzOUBF1xbYvTWy/o3cG9JFlQQ4Ach7RhZU8NrYHy5MJOJiVGpW+Cauo5DNU1RPcTkpuvv+qg7GOg5Oaqn+xPL1eelVdQKmFSA1Dgq4vH4OmsLHeuym7GqtEKVSonIpJarZ2zQ1QVZZOgVgCiwxuW+/BNSctC4uY0g0mQod07DCVAFZUbUpW6llKVhKl8YlTC38OxVK9RgUnlM2NCUc/FgQkTEUmKyQ7ZLIVchohm3npl/Vv7G+yxMJQE3a+MJUbGeo0qKn9uzUm9MmslTIfTb+L4DRm8028i4gEfJlJE9zkmO3TfqSgJSr2Ugx0/H0bUQ12gKtBi7k9pJn0oV9Q7RPdYN2EqGaqzdM8TACZRRLUckx2i/yjkMnQJ9sI/5wV0CfaCUqlEdGvD81Ze6R/K3iGJVCdhslTPk7nmPAGGEyb2XhGZB5MdogoY6gUyVm5oiOzfO0XsHapDqpowmWPOk7GEqS70XjG5orqCyQ6RGRlKgmraO8QkqG6p6pynqqyqq029V8bOYc5eLVN7ryo6B5M0ApjsEFlcTXuHKkqCANM+hJgwkY45eq9M3+KgRM0SqYp7r2rWM3av3FwrB62d0BnDRE+fTKhoo5v7hEqlgoeHB/Ly8qp9b6ytW7di4MCBNr3d9/0QZ22OsaZ/0ZqaMDExIqpewmSpZKyqc8MM/axLOdRZeuGHLqEzF1M/v5nsgMmOqe6HOG09RkO/eIDyv6Sq+svSUDkRWY7uj5GyvWhlH1eHNYY6+7e+d4PvmjD185vDWET3kbIrznR/YZk6nGZsrpGh8qpMzmbCRFQ1VdkotKosPdQ5cdUJLB31oNkSHlMw2SEig6oy18hYeVUmZ1sqYWIiRVR7CCjplUrcnIZ+YX5Wmy/EZIeILEaqhMnQHAFLJFKc80RUdQJKen6OpN80+HvAEpjsEFGtVp2EydBQnSV6noxNzKzKnCdzrSZi7xXVNTm3rPe9ymSHiO5b5uh5Amo+5wkwvJKltvZecYsDMgcfN0erXYvJDhGRBdQ0YarsHFL3XlVlY0wplmWzB6z2kgHw87j3vWUNTHaIiGyYpXqvjNU1Z6+WKb1X1e0Zs9ZEeEsldMZ60YwtSa8tiZ5uOvLsQWFW3cyQyQ4REZmVuRIpU3uvKjqHtVYOSpHQGepF86vklh1SD3X6mXmfHVMx2SEiovueuXrArJnQVdSLVtE1rT3UackdlE3FZIeIiKiOMpYYmeMclkzorE0uyVWJiIiIrITJDhEREdk0JjtERERk05jsEBERkU1jskNEREQ2jckOERER2TQmO0RERGTTmOwQERGRTWOyQ0RERDaNOygDEISS26ipVKpqPV+tViM/Px8qlQpKpdKcTatV7oc4GaNtYIy2gTHaBkvGqPvc1n2OG8NkB8CtW7cAAIGBgRK3hIiIiKrq1q1b8PDwMHpcJlSWDt0HtFotrl+/Djc3N8hkVb9vh0qlQmBgIK5evQp3d3cLtLB2uB/iZIy2gTHaBsZoGywZoyAIuHXrFgICAiCXG5+Zw54dAHK5HI0aNarxedzd3W32m7W0+yFOxmgbGKNtYIy2wVIxVtSjo8MJykRERGTTmOwQERGRTWOyYwYODg6YPXs2HBwcpG6KRd0PcTJG28AYbQNjtA21IUZOUCYiIiKbxp4dIiIismlMdoiIiMimMdkhIiIim8Zkh4iIiGwakx0zWLx4MZo0aQJHR0d06dIFR44ckbpJ1bZ//34MGjQIAQEBkMlk2Lhxo95xQRAwa9Ys+Pv7w8nJCZGRkbh48aI0ja2m+fPno1OnTnBzc4OPjw+GDBmCCxcu6NUpKChAQkICvL294erqitjYWGRnZ0vU4qpbunQp2rRpI27iFRERgW3btonH63p8hrz11luQyWSYPHmyWFbX45wzZw5kMpneV0hIiHi8rsen89dff2HUqFHw9vaGk5MTwsPDcezYMfF4Xf+906RJk3Lvo0wmQ0JCAgDbeB81Gg1mzpyJ4OBgODk5oVmzZpg7d67ePaskfR8FqpFvv/1WsLe3F7744gvh3Llzwvjx4wVPT08hOztb6qZVy9atW4Xp06cL69evFwAIGzZs0Dv+1ltvCR4eHsLGjRuF06dPC48++qgQHBws3L17V5oGV0N0dLSwYsUK4ezZs8KpU6eEgQMHCo0bNxZu374t1nn22WeFwMBAYdeuXcKxY8eErl27Ct26dZOw1VWzadMm4aeffhJ+//134cKFC8Lrr78uKJVK4ezZs4Ig1P34yjpy5IjQpEkToU2bNsKLL74oltf1OGfPni20atVKyMzMFL/+/vtv8Xhdj08QBOHmzZtCUFCQMGbMGOHw4cPC5cuXheTkZOHSpUtinbr+eycnJ0fvPUxJSREACHv27BEEwTbex3nz5gne3t7Cli1bhPT0dGHdunWCq6ur8OGHH4p1pHwfmezUUOfOnYWEhATxsUajEQICAoT58+dL2CrzKJvsaLVawc/PT3jnnXfEstzcXMHBwUH45ptvJGiheeTk5AgAhH379gmCUBKTUqkU1q1bJ9Y5f/68AEBITU2Vqpk1Vq9ePeHzzz+3ufhu3bolNG/eXEhJSRF69eolJju2EOfs2bOFtm3bGjxmC/EJgiC8+uqrQo8ePYwet8XfOy+++KLQrFkzQavV2sz7GBMTI4wbN06vbNiwYUJcXJwgCNK/jxzGqoGioiIcP34ckZGRYplcLkdkZCRSU1MlbJllpKenIysrSy9eDw8PdOnSpU7Hm5eXBwDw8vICABw/fhxqtVovzpCQEDRu3LhOxqnRaPDtt9/izp07iIiIsLn4EhISEBMToxcPYDvv48WLFxEQEICmTZsiLi4OGRkZAGwnvk2bNqFjx454/PHH4ePjg/bt2+Ozzz4Tj9va752ioiKsWrUK48aNg0wms5n3sVu3bti1axd+//13AMDp06fxyy+/YMCAAQCkfx95I9AauHHjBjQaDXx9ffXKfX198dtvv0nUKsvJysoCAIPx6o7VNVqtFpMnT0b37t3RunVrACVx2tvbw9PTU69uXYvzzJkziIiIQEFBAVxdXbFhwwaEhYXh1KlTNhEfAHz77bc4ceIEjh49Wu6YLbyPXbp0wcqVK9GyZUtkZmYiMTERDz30EM6ePWsT8QHA5cuXsXTpUkyZMgWvv/46jh49ihdeeAH29vaIj4+3ud87GzduRG5uLsaMGQPANr5PAeC1116DSqVCSEgIFAoFNBoN5s2bh7i4OADSf34w2aH7WkJCAs6ePYtffvlF6qaYXcuWLXHq1Cnk5eXh+++/R3x8PPbt2yd1s8zm6tWrePHFF5GSkgJHR0epm2MRur+KAaBNmzbo0qULgoKCsHbtWjg5OUnYMvPRarXo2LEj3nzzTQBA+/btcfbsWSxbtgzx8fESt878li9fjgEDBiAgIEDqppjV2rVrsXr1aqxZswatWrXCqVOnMHnyZAQEBNSK95HDWDVQv359KBSKcrPms7Oz4efnJ1GrLEcXk63EO2nSJGzZsgV79uxBo0aNxHI/Pz8UFRUhNzdXr35di9Pe3h4PPPAAOnTogPnz56Nt27b48MMPbSa+48ePIycnBw8++CDs7OxgZ2eHffv2YdGiRbCzs4Ovr69NxFmap6cnWrRogUuXLtnM++jv74+wsDC9stDQUHG4zpZ+71y5cgU7d+7E008/LZbZyvv48ssv47XXXsPIkSMRHh6O0aNH46WXXsL8+fMBSP8+MtmpAXt7e3To0AG7du0Sy7RaLXbt2oWIiAgJW2YZwcHB8PPz04tXpVLh8OHDdSpeQRAwadIkbNiwAbt370ZwcLDe8Q4dOkCpVOrFeeHCBWRkZNSpOMvSarUoLCy0mfj69u2LM2fO4NSpU+JXx44dERcXJ/7fFuIs7fbt2/jjjz/g7+9vM+9j9+7dy2398PvvvyMoKAiA7fzeAYAVK1bAx8cHMTExYpmtvI/5+fmQy/VTCoVCAa1WC6AWvI8WnwJt47799lvBwcFBWLlypZCWliZMmDBB8PT0FLKysqRuWrXcunVLOHnypHDy5EkBgPD+++8LJ0+eFK5cuSIIQsnSQU9PT+HHH38Ufv31V2Hw4MF1agmoIAjCxIkTBQ8PD2Hv3r16y0Hz8/PFOs8++6zQuHFjYffu3cKxY8eEiIgIISIiQsJWV81rr70m7Nu3T0hPTxd+/fVX4bXXXhNkMpmwY8cOQRDqfnzGlF6NJQh1P87//e9/wt69e4X09HThwIEDQmRkpFC/fn0hJydHEIS6H58glGwbYGdnJ8ybN0+4ePGisHr1asHZ2VlYtWqVWMcWfu9oNBqhcePGwquvvlrumC28j/Hx8ULDhg3Fpefr168X6tevL7zyyitiHSnfRyY7ZvDRRx8JjRs3Fuzt7YXOnTsLhw4dkrpJ1bZnzx4BQLmv+Ph4QRBKlg/OnDlT8PX1FRwcHIS+ffsKFy5ckLbRVWQoPgDCihUrxDp3794VnnvuOaFevXqCs7OzMHToUCEzM1O6RlfRuHHjhKCgIMHe3l5o0KCB0LdvXzHREYS6H58xZZOduh7niBEjBH9/f8He3l5o2LChMGLECL39Z+p6fDqbN28WWrduLTg4OAghISHCp59+qnfcFn7vJCcnCwAMttsW3keVSiW8+OKLQuPGjQVHR0ehadOmwvTp04XCwkKxjpTvo0wQSm1vSERERGRjOGeHiIiIbBqTHSIiIrJpTHaIiIjIpjHZISIiIpvGZIeIiIhsGpMdIiIismlMdoiIiMimMdkhIgIgk8mwceNGqZtBRBbAZIeIJDdmzBjIZLJyX/3795e6aURkA+ykbgAREQD0798fK1as0CtzcHCQqDVEZEvYs0NEtYKDgwP8/Pz0vurVqwegZIhp6dKlGDBgAJycnNC0aVN8//33es8/c+YM+vTpAycnJ3h7e2PChAm4ffu2Xp0vvvgCrVq1goODA/z9/TFp0iS94zdu3MDQoUPh7OyM5s2bY9OmTeKxf//9F3FxcWjQoAGcnJzQvHnzcskZEdVOTHaIqE6YOXMmYmNjcfr0acTFxWHkyJE4f/48AODOnTuIjo5GvXr1cPToUaxbtw47d+7US2aWLl2KhIQETJgwAWfOnMGmTZvwwAMP6F0jMTERw4cPx6+//oqBAwciLi4ON2/eFK+flpaGbdu24fz581i6dCnq169vvReAiKrPKrcbJSKqQHx8vKBQKAQXFxe9r3nz5gmCUHKn+meffVbvOV26dBEmTpwoCIIgfPrpp0K9evWE27dvi8d/+uknQS6XC1lZWYIgCEJAQIAwffp0o20AIMyYMUN8fPv2bQGAsG3bNkEQBGHQoEHC2LFjzRMwEVkV5+wQUa3w8MMPY+nSpXplXl5e4v8jIiL0jkVERODUqVMAgPPnz6Nt27ZwcXERj3fv3h1arRYXLlyATCbD9evX0bdv3wrb0KZNG/H/Li4ucHd3R05ODgBg4sSJiI2NxYkTJxAVFYUhQ4agW7du1YqViKyLyQ4R1QouLi7lhpXMxcnJyaR6SqVS77FMJoNWqwUADBgwAFeuXMHWrVuRkpKCvn37IiEhAe+++67Z20tE5sU5O0RUJxw6dKjc49DQUABAaGgoTp8+jTt37ojHDxw4ALlcjpYtW8LNzQ1NmjTBrl27atSGBg0aID4+HqtWrcIHH3yATz/9tEbnIyLrYM8OEdUKhYWFyMrK0iuzs7MTJwGvW7cOHTt2RI8ePbB69WocOXIEy5cvBwDExcVh9uzZiI+Px5w5c/D333/j+eefx+jRo+Hr6wsAmDNnDp599ln4+PhgwIABuHXrFg4cOIDnn3/epPbNmjULHTp0QKtWrVBYWIgtW7aIyRYR1W5MdoioVti+fTv8/f31ylq2bInffvsNQMlKqW+//RbPPfcc/P398c033yAsLAwA4OzsjOTkZLz44ovo1KkTnJ2dERsbi/fff188V3x8PAoKCrBw4UJMnToV9evXx2OPPWZy++zt7TFt2jT8+eefcHJywkMPPYRvv/3WDJETkaXJBEEQpG4EEVFFZDIZNmzYgCFDhkjdFCKqgzhnh4iIiGwakx0iIiKyaZyzQ0S1Hkfbiagm2LNDRERENo3JDhEREdk0JjtERERk05jsEBERkU1jskNEREQ2jckOERER2TQmO0RERGTTmOwQERGRTWOyQ0RERDbt/wFuJoTiORGD2wAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["train_loss_array = np.load(f\"/content/drive/MyDrive/P3DCV/losses/train/difffmap_1.76573.npy\")\n","val_loss_array = np.load(f\"/content/drive/MyDrive/P3DCV/losses/val/difffmap_1.76573.npy\")\n","epochs = np.arange(1, len(train_loss_array) + 1)\n","\n","plt.plot(epochs, train_loss_array, label='Train Loss', marker='o')\n","plt.plot(epochs, val_loss_array, label='Validation Loss', marker='o')\n","\n","plt.title('Train and Validation Loss Curve')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LI4-Yjk7iqPI"},"outputs":[],"source":["#test loops\n","for epoch in range(n_epoch):\n","    train_losses = []\n","    eval_losses = []\n","    faust_losses = []\n","    metrics = []\n","    # Training single Epoch\n","    for i, data in enumerate(train_loader):\n","        vertsA, vertsB = data\n","        batch = vertsA.shape[0]\n","        vertsA = vertsA.to(device).to(torch.float32)\n","        vertsB = vertsB.to(device).to(torch.float32)\n","        cat_verts = torch.cat([vertsA.transpose(1,2), vertsB.transpose(1,2)], dim=0)\n","        optim.zero_grad()\n","        basisNet = basisNet.train()\n","\n","        # Obtaining predicted basis\n","        cat_preds, _, _ = basisNet(cat_verts)#cat_pred.shape (B, n_points, k)\n","\n","        basisA = cat_preds[:batch, :, :]\n","        basisB = cat_preds[batch:, :, :]\n","        # Computing optimal transformation\n","        pseudo_inv_A = torch.pinverse(basisA)\n","        C_opt = torch.matmul(pseudo_inv_A, basisB)\n","        opt_A = torch.matmul(basisA, C_opt)\n","\n","        # SoftMap\n","        dist_matrix = torch.cdist(opt_A, basisB)\n","        s_max = torch.nn.Softmax(dim=1)\n","        s_max_matrix = s_max(-dist_matrix)\n","\n","        # Basis Loss\n","        eucl_loss = torch.sum(torch.square(torch.matmul(s_max_matrix, vertsB) - vertsB))\n","\n","        #metric = torch.mean(torch.square(s_max_matrix - torch.identity(s_max_matrix.shape[0]))) # measuring how well we are approximating the ground-truth correspondence\n","        # Back Prop\n","        eucl_loss.backward()\n","        optim.step()\n","        train_losses.append(eucl_loss.detach().item())\n","        #metrics.append(metric.detach().item())\n","    ave_train_loss = sum(train_losses) / len(train_losses)\n","    #ave_metric = sum(metrics) / len(metrics)\n","    print(f\"training loss for epoch{epoch}:\", ave_train_loss)\n","    #print(f\"training metric for epoch{epoch}:\", ave_metric)\n","    torch.save(basisNet.state_dict(), f\"/content/drive/MyDrive/P3DCV/models/difffmap_resampled_epoch_{epoch}.pt\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BCOO62564bKB"},"outputs":[],"source":["'''\n","\n","    # Validation\n","    with torch.no_grad():\n","        eval_loss = 0\n","        for data in tqdm(dataset_test, 0):\n","            points = data[0]\n","            points = points.transpose(2, 1)\n","            points = points.cuda()\n","            basisNet = basisNet.eval()\n","            pred, _, _ = basisNet(points)\n","            basis_A = pred[1:,:,:]; basis_B = pred[:-1,:,:]\n","            pc_A = points[1:,:,:]; pc_B = points[:-1,:,:]\n","\n","            pseudo_inv_A = torch.pinverse(basis_A)\n","            C_opt = torch.matmul(pseudo_inv_A, basis_B)\n","            opt_A = torch.matmul(basis_A, C_opt)\n","\n","            dist_matrix = torch.cdist(opt_A, basis_B)\n","            s_max = torch.nn.Softmax(dim=1)\n","            s_max_matrix = s_max(-dist_matrix)\n","            eucl_loss = torch.sum(torch.square(torch.matmul(s_max_matrix, torch.transpose(pc_B,1,2)) - torch.transpose(pc_B,1,2)))\n","            eval_loss +=   eucl_loss.item()\n","\n","        print('EPOCH ' + str(epoch) + ' - eva_loss: ' + str(eval_loss))\n","\n","        # Saving if best model so far\n","        if eval_loss <  best_eval_loss:\n","            print('save model')\n","            best_eval_loss = eval_loss\n","            torch.save(basisNet.state_dict(), '%s/basis_model_best.pth' % (outf))\n","\n","        train_losses.append(train_loss)\n","        eval_losses.append(eval_loss)\n","\n","        # Logging losses\n","        np.save(outf+'/train_losses_basis.npy',train_losses)\n","        np.save(outf+'/eval_losses_basis.npy',eval_losses)\n","'''"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}